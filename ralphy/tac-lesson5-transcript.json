{
  "video_id": "close-the-loops",
  "url": "https://agenticengineer.com/tactical-agentic-coding/course/close-the-loops",
  "title": "Close The Loops: More Compute, More Confidence",
  "channel": "Agentic Engineer",
  "duration": 3233,
  "language": "en",
  "language_name": "English",
  "is_auto_generated": false,
  "extracted_at": "2026-01-14T03:14:51.315Z",
  "transcript": "Welcome to Tactical Agentic\nCoding, Lesson 5. Let's\nbe brutally honest with\nourselves. As engineers, we\noften make the mistake\nof thinking the most\nvaluable asset we create\nis code, architecture, systems,\nplans, or features. This\nis wrong. Our most\nvaluable contribution is the\nexperience we create for\nour users. That means\none of the most\nvaluable things we can\ndo is making sure\nthat all of the\nengineering stuff, the code\narchitecture features, do what\nthey're meant to do\nin the first place.\nHow do we do\nthat? You guessed it.\nWe test, we validate,\nwe close the loop\non the work done\nso we know for\na fact that the\nexperience we designed for\nour users is what's\nin our users' hands.\nAgentic coding presents us\nwith a massive opportunity.\nThe opportunity to have\nyour agents test on\nyour behalf like you\nnever could at scales\nyou never will achieve.\nThis is the gift\nof generative AI. This\nis the gift of\nthe agent architecture. There\nis one question that\nguides this lesson that\nunlocks this opportunity for\nyou. This question is\nkey to unlocking one\nof the most powerful\nleverage points of Agentic\ncoding. This leverage point\ngives your agents the\nability to save you\nhundreds of hours by\nautomating tests and review\nworkflows that would otherwise\ncost your time and\nattention. All you have\nto do is answer\nthis one question. Given\na unit of valuable\nwork that's production ready,\nhow would you, the\nengineer, test and validate\nthis work? If you\ncan answer this question\nfor every class of\nwork that your code\nbase handles and then\nencode the answers into\na command or tool\ncall, you will fly\nwhile other engineers run.\nIt's that simple. And\ndepending on your staging\nand production system, it's\nthat complex. This is\nwhere code-based architecture comes\ninto play as a\nmassive critical leverage point.\nNet new codebases\nhave a massive advantage\nhere. So let's go\nahead and answer the\nquestion. When we ship\ncode to production, how\ndo we validate that\nour work has shipped\nsuccessfully? If you think\nthrough this, you'll find\na concrete flow of\nsteps and tools that\nyou go through to\nknow that your code\nbase has shipped successfully\nin production without errors.\nFor example, you might\nrun your linter, you\nmight execute unit tests,\nyou'll run UI tests,\nmaybe your CI CD,\nruns your integration tests,\nyou likely build or\ncompile your application. Maybe\nyou look for specific\nlog messages in Datadog\nto confirm your features\nlive, or maybe you\ncheck Sentry to look\nfor a lack of\nerror messages. If you're\ntraining models or doing\ndata science work, maybe\nyou run a custom\nevaluation that ranks your\nartifacts performance. And maybe\nyou have an LLM\nas a judge workflow.\nAnd here's the big\none for many engineers.\nYou probably opened the\nbrowser and clicked through\nyour new feature. What\na waste of\ntime, okay? These are\nall feedback loops. Let\nme explain why this\nis a waste of\ntime, okay? These are\nthings that you and\nI will do less\nand less and less\nas we scale our\nagentic systems. These are\nall feedback loops you\ncan now hand off\nto your agents. This\nis all work you\ndon't have to do\nanymore, but likely still\nare. Why are we\nstill doing this work?\nIt's because we're missing\ntesting as a critical\nleverage point of Agentic\ncoding in your code\nbase. That brings us\nto the lesson five\ntactic. The tactic here\nis dead simple. It\nbuilds on a principle\nof AI coding you've\nlikely heard before. The\nlesson five tactic is\nalways add feedback loops.\nYour work, my work,\nany engineer's work is\nuseless unless it's tested.\nThe ultimate test will\nalways be your users.\nThe next best test\nis you. Well, it\nused to be. Now,\nthe next best test\nis an army of\nagents validating your entire\ncodebase with regression\ntests and most importantly\nwith end-to-end tests. I'm\nnot saying stop testing\nor stop reviewing. I'm\nsaying start handing off\nthis responsibility and start\nteaching your agents to\ntest. Why should we\ndo that? Why should\nwe hand off more\nto our agents. Of\ncourse, when you do\nthis, you create closed\nloop feedback systems where\nyour agent can execute,\nvalidate, and reflect on\nthe work done in\na loop until the\njob is done. By\nteaching your agents to\ntest, you continue to\ndial up the autonomy\nlevel of your agents\nin your codebase.\nWe continue the trend\nof building out the\nnet new Agentic layer\naround your codebase.\nThis is us building\nthe system that builds\nthe system. Keep this\nidea in mind because\nthis is the differentiating\nfactor between Agentic Engineers\nand engineers of the\npast. Let's be clear\nin our terminology here.\nWhen I refer to\nin-loop Agentic coding, I\nmean you sitting right\nhere, prompting with your\nagent back and forth.\nOutloop is of course\na high level prompt\nrunning through the Peter\nsystem that fires off\non your isolated device,\non your isolated machine.\nWe're going to fire\noff an AFK agent\nin this lesson that's\ngonna run our new\nfull pipeline. Now, closing\nthe loop is letting\nyour agent operate on\nwork, calling a command\nor a tool to\nget feedback on the\nsuccess of the work.\nYou then take the\nfeedback and your agent\ncontinues to build until\nthe feedback is positive.\nThis is what it\nmeans to close the\nloop. When you do\nthis, you let the\ncode write itself. You\nlet the agent operate\nwith the right information\nso well that it\ncloses the loop. Now\nwith testing, one of\nthe most heated debates\nin the engineering world\nhas come to an\nend. Now engineers that\ntest with their agents\nwin. Full stop, zero\nexceptions. This is because\nthe value of tests\nare multiplied by the\nnumber of agent executions\nthat occur in your\ncodebase. Testing is\none of the highest\nthrough agent leverage points\nof Agentic coding, you\ncan use. Why is\nthat? It's because it\nlets your agent close\nthe loop and self-validate.\nIt lets your agent\nknow that the work\nwas done right. This\nleverage point is so\nimportant. It has an\nentire lesson to itself.\nAll right. And this\nmakes sense, right? If\nyour auth tests are\npassing, you can be\nmore confident your authentication\nsystem is working. If\nyour UI tests for\nyour chat support system\nare passing, you can\nbe more confident that\nyour chat support system\nis working. If your\nend-to-end tests on your\nstaging environment are passing,\nyou can be very\nconfident every system is\nworking. And with every\npassing set of tests,\nyou free your context\nwindow and you stop\nsecond guessing so you\ncan focus on what's\nnext for your users.\nIt's no different for\nyou. or for your\nagent. If your agent\nsees something wrong with\nthe auth test, you\ncan't ship to production.\nIt has to dive\ninto the system and\nfix it. Test, let\nyour success scale. Successful\ncodebases will grow.\nAs you're testing every\nnet new feature with\nyour agents, you can\nscale with confidence. With\ndone. Looks great. And\nthen it's going to\nreport a bunch of\nstuff to us here.\nWhile this is running,\nlet's discuss a couple\nthings. So there are\nmany ways to test.\nIn this lesson, we're\ngoing to cover common\nfeedback loops you can\ngive your agents use\nthese as guides to\nunderstand what you can\ndo but while you're\nlooking at this while\nwe're working through this\ndon't limit yourself remember\neverything is one tool\ncall away tools are\njust functions and functions\ncan do anything we'll\nwork through these capabilities\nusing in-loop Agentic coding\nlike we are here\nright we are actively\nprompting back and forth\nto our agents this\nis for presentational purposes\nas you progress You\nwanna be doing less\nand less of this.\nYou wanna stay out\nthe loop, kicking off\nworkflows that run in\nan agent environment. And\nso speaking of that,\nlet's go ahead and\nfire off a complete\nworkflow where our agent\nis gonna run the\nplan, build and test\nstep of the software\ndevelopment lifecycle. It's gonna\ntake some time to\nrun. It's gonna build\na full feature and\ntest it for us.\nSo let me just\ngo ahead and kick\nthis off. So you\ncan see our agents\nprimed here. That's great.\nIf we look at\nour ADWs here, We\nhave a new set\nof ADWs we're gonna\nbreak down later. What\nwe wanna do right\nnow is just kick\noff the full flow\nthat we have built\nup up to this\npoint. You can see\nwe've improved the application\nstructure. Right now, all\nwe wanna do is\nspin up a new\nOutloop AFK agent using\nthe PIDA framework we\ndiscussed in the previous\nvideo. So how am\nI gonna kick that\noff? I'm gonna open\nup GitHub. And I'm\njust going to kick\noff a brand new\nissue here. Alright, so\nI'm gonna hit new\nissue. This is my\ntrigger. And so I'm\ngoing to write a\nprompt and put here\nand let's go ahead\nand do this random\nnatural language query. Let's\nalso open up the\napplication to get a\nrefresh. And what we\nwant to do here\nis just add a\nsimple button. We want\nour agent to handle\nall this work for\nus. We don't need\nto be doing any\nof this feature building\nwork. But we're going\nto build a new\nfeature where we click\na button and we\nwant to have a\nbrand new natural language\nSQL query created for\nus that helps us\njust get started with\nquerying in our test\napplication here. All right,\nso I'm gonna type\nthis ADW plan build\ntest. So this specifies\nthe ADW we're running.\nI'm gonna say feature.\nI'll write the prompt\nhere. So create a\nnew button. All right,\nso I have a\nkind of high, maybe\nmid-level prompt here detailing\nsome work. And this\nagent, this Outloop AFK\nagent system is going\nto handle this for\nus agentically. So this\nis gonna run on\nour agent device here.\nOur agent has full\ncontrol over this machine.\nIf I open up\nscreen, you can see\nexactly what's going on\nhere. So I have\nmy agent's machine right\nhere, dedicated box just\nfor my agents. I\ncan close this. I\ncan open up code\nand you can see\nhere we are listening\nto our GitHub web\nhooks. This is the\ntrigger that's going to\nkick off this agentic\nworkflow once the prompt\ninput comes in. Let's\ngo ahead and kick\nthis off. And then\nyou're going to see\nthis kickoff as that\nweb hook event comes\nin. There it is.\nThere's a lot of\nwork that's going to\nhappen here in the\nbackground. You can see\nour agents getting to\nwork there. We've made\nsome improvements. We'll get\naround to these. This\nis going to be\na long running Outlook\njob. We're going to\nhave a slew of\nchained together agents operating.\nfire this off inside\nof our agent here.\nSuper simple AI coding\nchange, right? We're commenting.\nAnd then we're going\nto run rough. As\nyou can see, rough\npicked up an error.\nWe now have a\nunused import. Rough is\nlooking at that change.\nIt's going to provide\ninput back to our\nagent via standard out.\nOur agent picks up\non that as it's\ncontext window and then\nreruns rough check. It's\ndone. And now it's\ncompleted the work that\nwe wanted. Okay. So\nno issues. This is\nthe simplest closed loop\nthat you can imagine.\nAll right. Now the\nstructure of this is\nvery important. This is\nunlike a standard prompt.\nThis prompt instructs the\nagent to validate itself.\nIt creates a small\nloop system. Okay. So\nlet's scale this up\nand let's add another\nfeedback loop. Right. So\nI'm going to run\nslash clear here. Let's\nreplace this prompt here\nwith an updated prompt.\nWe're going to update\nthe server file, remove,\nthe insights endpoint. So\nif we go here,\nif we collapse everything\nhere, we have this\nunused import here. Let\nme go ahead and\ncollapse even further. We\nhave this insights right\nhere. And so this\nis not being used.\nWe want to get\nrid of this. And\nthen we're going to\nmove our load and\ncall to load on\nenvironment variables right before\nit's import. So there's\nthe request. And then\nhere's our validation validate\nmultiple things. So we're\nopening up a couple\nloops here that our\nagent now has to\nclose. Okay. It needs\nto run these commands,\nlook for errors. If\nthere are resolve them.\nAll right. So rough\npy test and then\na classic Python compile.\nOkay. Again, we're just\ngoing to copy this.\nWe have a new\nclear dot instance here.\nWe're going to run\nthis and let's see\nwhat our agent does.\nSo we have a\nlinter, we have a\nunit tester and we\nhave a compilation command.\nAll right, so we're\nstarting to stack up\nfeedback mechanisms for our\nagent, okay? This is\nstill just the beginning\nof what we can\ndo with this. Here\nwe go, let's watch\nwhat our agent does\nhere. So we have\nthis insights endpoint here,\nand this is gonna\nget removed momentarily. So\nit moved the load,\nand now it's gonna\nrun rough. There it\nis. So there's an\nissue here, right? We\ncan't have load env\nabove an import. Ruff\nfound this. So it\nwas passed right into\nthe standard in of\nour agent and our\nagent is going to\nmove this. It's gonna\nresolve the issue. There\nit is. Our agent\nadded some duplicate imports.\nThis happens. Okay. Now\nit's gonna run Ruff\nagain. So check this\nout. It ran it\nagain, right? It's validating\nits test again. And\nnow it's continuing to\nthe next step. Now\nit's closing the next\nloop. Okay. So now\nwe know that our\ncode is formatted. Here\nwe go, PyTest. And\nso PyTest is running.\nLooks like there is\nan error here. Let's\nsee what the error\nwas and what our\nagent does. Okay, so\nit sees that that\nline is commented out\nand it picked up\non the bug that\nwe created in our\nlast prompt, right? So\nit sees that the\nvalidate SQL query has\nbeen commented out. I\nneed to uncomment this\nand call a function.\nSo there's that import\ncoming back in and\nthere's that comment getting\nresolved. So without knowing\nwhat we previously did,\nright, thanks to the\ncontext window clearing, it\nfixed this issue and\nmade sure that all\nthe pie tests ran.\nAnd this is important.\nYou have to make\na rule inside of\nyour codebased architecture.\nWhat's more important, the\ncode or your tests?\nThe answer should be\nyour tests. Your tests\nshould be the rule\nof law in your\ncodebase. If your\ntests aren't right, focus\non having your agents\nupdate the test so\nwould. Right. Let's scale\nit up again. all\naround slash clear just\nto keep the agents\ncontext window fresh. And\nnow we're gonna run\nthis prompt. So let\nme paste this and\nlet's break this down\na little bit. So\nwe've now turned this\ninto a full reusable\nprompt that our agent\ncan run, okay? This\nhas gone from a\ntoy prompt, right? A\ntoy high level prompt\nto now a mid-level\nreusable prompt that you\nwould want to embed\nin your codebase.\nAll right, so how\ndoes this work? What\ndoes this look like?\nWe have the purpose\nof the top validation,\nvalidate your work. You\ncan see our same\nbackend checks. I'm running\na few front end\nvalidation checks here. If\nyou have some type\nof front end VITES\nor Jest tests, this\nwould be the place\nto run those. That'll\nincrease the confidence you\nhave that your front\nend codebase is\ncorrect. And it'll give\nyour agents the ability\nto correct the issues\nas they come up.\nAnd then we have\nsomething really cool here,\nsomething brand new. So\nI'm gonna just copy\nall this and let's\ngo ahead and run\nthis. And then we'll\ntalk about what exactly\nis happening here. So\nlet's fire this off.\nOur agent's gonna run\ntop to bottom. You\ncan see I have\na couple instructions here.\nRun in order top\nto bottom. If you\nwant to take any\nissues, stop and resolve\nthem immediately. Then rerun\nevery validation step, okay?\nIt's not good enough.\nto resolve this once\nand that resolution causes\nsomething to break, right?\nYou need them all\nto run from start\nto finish. What's happening\ndown here. We are\nrunning the Playwright MCP\nserver. Of course, valuable\nlinks is going to\nbe available in your\nloot box, detailing all\nof these services, detailing\nall the tech we\nuse in this lesson.\nWe're running the Playwright\nMCP server to run\nthe simple validation workflow.\nSo we're going to\nopen the browser. We're\ngoing to take a\nscreenshot. We're gonna run\na query. Our agent\nis gonna type for\nus, right? It's gonna\noperate the browser, query,\ntake a screenshot, and\nthen read both images\nand verify that the\nnatural language query was\nexecuted correctly, all right?\nconfirm seven results are\nreturned. So this is\na very specific test,\nright? It validates a\nprecise result from our\ncodebase given the\nstate of our code\nbase. All right. So\nyou can see it\nhere working through. If\nwe scroll back up,\nyou can see those\ntests are passing, right?\nThe codebase is\nin a good state\nright now. So that's\ngreat. That's a CD\ninto the right directories.\nThat's totally cool. Nothing\ninteresting happening there. TSC\nemit. So we have\nno front end TypeScript\nissues. Okay. So once\nagain, We've increased the\nconfidence just a little\nbit. And then we're\ngoing to compile our\nfront end application here.\nThere it is, Bunrun\nbuild. Our front end\nis now built for\nproduction. Again, we're stacking\nthe confidence. We know\na little bit more,\na little bit better\nthat things are going\nto work. So here\nwe go. Now Playwright\nis executing. Check this\nout. If you haven't\nplayed with this yet,\nif you're not aware\nof this, this is\nhuge. Browser control is\nanother tool that you\ncan use to validate\nwork. And keep in\nmind, I'm passing in\na local URL. When\nyou're building these systems,\nwhen you're setting up\nyour ADW to test\nyour system and to\nvalidate your work, you\nmight point this at\na staging system or\neven a production test\naccount so that you\nknow that the work\nis complete, right? And\nnotice what every one\nof these things are\ndoing for us. We're\ngiving our agents the\nability to validate that\nthe work is complete,\nokay? This helps us\nYou do not build\nin these closed loop\nsystems with these powerful\nclosed loop prompts. You\nare going to be\nwasting more time. Your\nagents can be solving\nissues if you just\ngive them a little\nmore feedback and then\na little more and\na little more until\nthey'll be able to\nsolve a massive, massive\nshot of issues, massive\nbuckets of issues because\nthey have feedback loops.\nAnd so the whole\npoint of what we're\ndoing here is save\nyourself some more time,\nbecome more of an\nAgentic Engineer, lean into\nthe future. Don't lean\ninto the present, don't\nlean into the past,\nlean into the future,\nokay? Hand off more\nresponsibility to your agents,\nnot less. We're increasing\nconfidence, we're increasing the\nfact that we know\nour agent has shipped\nwork for us with\nevery test we add,\nwith every validation. And\nyou know, just to\nmention it again, you\nknow, what do we\nhave here? And we\nhave one, two, three,\nfour, five, and then\nsix. closed loop tests\nand really, you know,\npie test is a\ncrap ton more than,\nyou know, just one,\nbut we can put\nit under one bucket,\nright? You know, with\nthis, we're adding tons\nof confidence to our\nagents. Now, let's move\nforward from this, right?\nWe now know that\nwe can add these\npowerful closed loop prompts\nwhere our agents can\nvalidate their own work.\nThey can know that\nwhat they've done is\ncorrect, just like you\nor I can by\nrunning tests, by building,\nby compiling, by looking\nat the browser, right?\nWe're handing this off\nto our agent. So\nhow do we take\nthis a step further?\nHow do we embed\nthis into the Agentic\nlayer, right? The new\nAgentic layer that we're\nbuilding around all of\nour codebases. How\ndo we teach the\nagents in our code\nbase how to test?\nWe can of course\nembed them into our\ntemplates, into our reusable\nprompts. So if we\nopen up dot Claude,\ngo into commands, you\ncan see we have\na growing list of\nreusable natural language solutions\nthat our Agentic coding\ntools can use to\nship to solve all\ntypes of problems that\nwe run into in\nour codebase. Okay.\nFor instance, we have\na bug prompt. Let's\ndial into that and\nlet's fix this issue,\nright? If I go\nhere and I hit\nrefresh, We're getting the\nnew tables. Our test\ndeleted a user table.\nThey also added this\none and they added\nthis, you know, drop\nuser. So we have\na bug in the\ncodebase. Okay. Let's\nresolve that. Of course,\nwe're not coding anything,\nright? We've stopped coding.\nWe know that that's\na key tactic. It's\nthe first tactic of\nAgentic coding. What we're\ndoing here is adding\nfeedback loops in to\nour templates. Okay. So\nonce again, we're templating\nour engineering. If we\nsearch end to end\nin this bug file,\nlet me close these\nother files. And let\nme actually just, Where\nare we here? Let\nme revert the code\nbase, clean everything up.\nLet's remove these two\nfiles and let's proceed.\nAll right, so just\nclean up the code\nbase real quick there.\nSo what are we\ngonna do here? We\nare going to search\nend to end. So\ncheck this out, important.\nIf this bug affects\nUI or user interactions,\nwe're gonna add a\ntest. Let me backtrack\na little bit, right?\nWe have our bug\nreusable meta prompt. All\nright, so we covered\nthis in our previous\nlessons. If we collapse\neverything here, you can\nsee all of the\nsections and the key\npart is just the\ntop. This is a\nmeta prompt. It's a\nprompt that creates a\nprompt. We're templating our\nengineering into some key\nreusable prompts into the\ncodebase so that\nwe can solve problems\nvery quickly like this.\nSo let's hop back\nto that instance. Let's\nclear. We always want\nfresh agent instances. We\ndon't want any context\npollution and then I'll\nrun slash bug. And\nlet's look at the\nvariables that this prompt\ntakes in. So we\nneed an issue number.\nWe'll just type one,\none, one here. We\nneed an ADW ID.\nThis is running on\nits own. We're running\nin the loop. So\nthere's no AI developer\nworkflow ID. We'll just\ntype ADW one, one,\none. And then we\nneed our actual prompt,\nright? The issue that\ngets passed in here.\nWe need to resolve\nthis issue. We have\na couple of tests.\nthat are creating some\nreal junk data in\nour production database, right?\nWe're running in SQL\nlite, we can run\nthese in memory. Our\ntests shouldn't be operating\nin our quote unquote\nlive production database. So\nwe'll run this prompt.\nBasically, I'm just describing\nthis at a high\nlevel. Then our bug\nmeta prompt is gonna\npick this up, it's\ngonna create a plan\nfor us, okay? What\nhave we done here\nwith this bug plan,\nright? Because we had\nthis before, how have\nwe improved this? So\na couple things, you\nmay have noticed even\nbefore in our previous\nlesson, If you look\nat the plan format,\nthe template that our\nagent is filling out\nfor us, if we\nscroll down here, you\ncan see we have\nvalidation commands and you'll\nnotice something here. It's\ntemplated in to the\nplans that get generated\nby our agent. We\nhave validation commands, right?\nSo with every plan\nwe build, with every\nbug we solve, with\nevery feature we build,\nwe are going to\nhave validation commands baked\nin by default. Okay.\nWhen we talk about\nscaling our impact with\nAgentic coding, when we\ntalk about always adding\nfeedback loops, I mean\nit always add feedback\nloops and you can\ndo this. You know,\nthis isn't a huge\ncumbersome task. You encode\nit, you template your\nengineering into your reusable\nprompts so that your\nagents can pick up\non the work and\nbuild the way you\nwant to build over\nand over and over.\nAnd the way you\nwant to build is\nwith high confidence. You\nwant to know that\nthe feature has shipped.\nOkay. We also have\nanother line in here,\nright? A template inside\nof the prompt. If\nyou create an end\nto end test, include\nthe following validation steps.\nSo we're just saying,\nyou know, specifically add\nthis in addition to\nall the other validation\ncommands that you'll use\nto validate with 100%\nconfidence, right? We're just\ndoing some prompting here\nthat the issue has\nbeen fixed with no\nregressions. All right. So\nsuper powerful stuff here.\nYou can see our\nagent is It's still\nrunning, it's looking for\nthis issue. It looks\nlike it's gonna find\nit pretty soon here.\nWhile we're doing this,\nwhile this is running,\nwe have an agent\nrunning here. We have\nan agent running here.\nLet's fire up another\nissue that I noticed\nthat I wanted to\nfix in this lesson.\nWhen we run queries,\nso if I say\nselect five products, the\ninput field does not\nget disabled. So you\ncould keep querying over\nand over, right? This\nis like a common\nUI thing. This is\nnot something that you\nor I need to\nsolve ever again. Right,\neven with just a\nsimple in loop prompt,\nwe could fire that\noff and it will\nresolve this. But we\ndon't just wanna know\nthat the issue is\nfixed, we wanna know\nthat the issue is\nfixed and we want\nour agent to be\nconfident about that. Okay,\nso we can fire\noff another agent here.\nand we can have\nour agent fix that\ninput issue. So I'm\njust gonna copy this\nin here, bug, give\nit an ID ADW222,\ndisable the input query\nwhen the query is\nrunning and add a\ndebounce request, all right?\nSo we can fire\nthat off while our\nother agent is working\nfor us. And let's\ngo ahead and see\nif that test has\ncompleted. There we go.\nSo it's finished. So\nthis is our plan\nthat it got generated,\nright? So just to\nmake it super clear\nhere, specs, issue 111\nADW, and you can\nsee the rest of\nthe ID there. This\nis our plan step\nin the software development\nlifecycle. We've automated this\nand we've solved the\nproblem of scaling our\nimpact with our plans,\nright? Our plan velocity\nis through the roof\nnow because we've templated\nour engineering. How does\ntest plan of this?\nYou can see here\nwe have a great\nbug description that's detailing\nthe exact issue, right?\nIt expanded on the\nhigh level prompt that\nwe sent in and\nturned it into a\ndetailed low level spec.\nGreat planning is indeed\ngreat prompting. And we\nhave a prompt to\nautomate writing great plans\nfor our codebase,\nright? So it's very\nspecific. There's our reproduce\nsteps. And you should\nbe able to see\nright our agent has\ncorrectly identified that you\nshould be able to\njust look at the\ncodebase and look\nat the tables to\nsee these error tables\nso our agent can\nof course just execute\nthis you can see\nno new files and\nif we search end\nto end there isn't\nany hint of the\nthese end to end\nfiles right this is\na back-end specific change\nso our if statement\nhere right to kick\noff our end to\nend playwright tests that\ndid not get activated\nbecause our agent correctly\nidentified the fact that\nthis bug does not\naffect the UI. But\nso in the generated\nplan, we can search\nthrough this. And if\nwe, you know, look,\nhere's all the step-by-step\nplay. But if we\ncome down here to\nthe bottom, the validation\ncommands, you can see\neverything our agent will\ndo to validate that\nthe bug was fixed.\nAll right, so we\nhave a couple SQLite\nthree runs. We have\nUVPyTest, right? Testing that\nspecific file that had\nthe issue. We have\ndiff before and after\ntable. So it's actually\nwriting to standard out\nbefore, after, then we\ndiff it, okay? And\nso these are all\nvalidations, right? We've encoded\nthe act of validating\ninto our prompts. This\nis very, very powerful.\nWhen our agent inevitably\nmakes a mistake, its\nvalidation command will create\na closed loop, several\nclosed loops, right? You\ncan see, We're testing\na multitude of things\nhere. It'll create a\nclosed feedback loop for\nour system to correct\nitself. All right, so\nlet's look at how\nour other plan has\ndone. You can see\nwe got this input\ndebounce plan. While we're\nwaiting here, let me\njust go ahead, let's\nkick off the implementation\nof this. And so\nwe can implement this\nwith slash implement and\nwe'll pass in this\nfile. All right, and\nso that's all we\nneed. Oh shoot, we\nhave a overload. Let's\nrun the implement again.\nSo we've cleared and\nnow we're going to\nimplement. Hopefully this works.\nThere we go. So\nthat looks great. All\nright, so now our\nplan is going to\nget implemented here. And\nif we open up\nthat implement command, we've\nlooked at this in\nprevious videos. This is\na higher order prompt\nthat takes a plan\nin as an argument\nand then has a\ncouple instructions around the\nplan. All right. So\nthat's that this engineering\nresource is now going\nto get kicked off\nand create the solution\nfor us. So let's\nlook at our input\ndebounce. And what I\nwant to point out\nhere is the difference\nbetween these two plans,\nright? Because we've template\nor engineering and we've\nadded feedback loops into\nthe templates. We can\nscroll down. We can\nsee we have this\nnew file. in our\nsolution for our disable\ninput debounce query. All\nright, so when we\nmake a query, we\nwant to disable the\ninput field so you\ncan't spam it, right?\nThis is a classic\nfront end problem. This\nis great. So we\nhave this end to\nend test that's gonna\nget built here as\na new file. Everything\nelse is just changes\nto existing files, okay?\nAnd if we scroll\ndown here, one of\nthe tasks is create\nend to end test,\nright? So it's saying\nread this file and\nthis file. and then\ncreate an end-to-end test.\nAnd we're gonna dig\ninto what the end-to-end\ntests look like, but\nit's very similar to\nour Playwright prompt that\nwe looked at before,\nright? Our closed loop\nprompt, you can see\nwe have clean user\nstore here that describes\nwhat we're testing exactly.\ntest steps we have\nthis powerful information dense\nkeyword verify we have\na variable here that\nwe're referencing from a\nhigher order prompt and\nthen we have our\nsuccess criteria these things\nmust be true in\norder for this test\nto pass and then\nwe'll dive into the\ntest end to end\nlater on in this\nlesson but you can\nsee here this test\nthanks to us templating\nour engineering practice of\ntesting into our bug\nreusable meta prompt this\nis now agentically going\nto create this end-to-end\ntest for us. And\nwe can of course\nspin this up as\nwell. I'm gonna copy\nthe reference to this,\ngo to this terminal,\nI'm gonna clear it,\nright? Even though this\nwill probably be somewhat\nuseful to have the\ncontext of it being\nbuilt, always run fresh\nagents is a good\npractice and it moves\nyou toward true off\ndevice Agentic coding where\nyou're gonna have multiple\nagents picking up context,\npicking up work from\nprevious agents with fresh\nfocus context windows, right?\nSo we'll type clear\nimplement and I'll pass\nin this plan and\nlet's go ahead and\nget to work on\nthat. And actually I'm\ngonna hold off on\nthis. I forgot I'm\nrunning in the loop.\nI'm running in loop\nAgentic coding. So this\nmight modify files. This\nagent is kind of\noperating my device. We're\nnot running on Git\nwork trees or anything,\nright? Unlike my isolated\ndevice here that can\ndo whatever it wants,\nright? We can run\nagent can take over\nthe device here. I\nhave to run one\nat a time, okay?\nAnd this is one\nlimitation of in loop\nAgentic coding. You're pretty\nmuch limited one agent\nunless you're conscious about\neverything that you're changing.\nOkay. This is great.\nThis is running. It\nlooks like we're going\nto implement that. This\nis going to build\nup the future. I'm\nnot super concerned with\nthe results of this.\nI mainly just want\nto showcase how our\nagent is going to\nrun these self-validating loops,\nright? So inside of\nthis plan, right in\nour issue one, one,\none, or fixing this\nvalidation command, right? You\ncan see our agent\nhas started the test\nto validate step. So\nit's going to first\nrun tests. and then\nit's gonna execute all\nof our regression validation\ntests, right? So it's\ngonna run all of\nthese commands next. And\nso this is super\npowerful. This is gonna\nkeep working and you\ncan see the closed\nloop system getting to\nwork, okay? I can\nsee that there are\nsome tables in the\ndatabase. I'm running thinking\nmode by the way\nwith the information dense\nkeyword in our implement\nhigher order prompt, right?\nA prompt that takes\na prompt as a\nparameter, right? An argument.\nYou can see I\nhave think hard. the\nClaude Code, Anthropic, Encoded\nInformation Dense Keyword. This\nis a principle of\nAI coding. There are\nkeywords that you can\ncreate or that you'll\nbe able to reference\nthat have more meaning,\nthat have some special\neffects. The Think Hard\nactivates the Cloud Series\nmodels thinking modes, right?\nSo you can see\nthat happening here. Elec\nalready knew that, but\nit's important to mention.\nWe are now running\ntests and we're executing\nour validation commands. So\nif you open up\nthe validation commands, you\ncan see all these\ntests are running. They're\npassing. That looks great.\nAnd this agent is\ngoing to run to\ncompletion here. Let's go\nahead and move on\nto the next abstraction\nlevel. We've You know,\nstarted with our simple\nclosed loop prompt. We're\njust running a linter.\nWe then added entire\nbackend tests with PyTests\nand we're compiling the\nbackend now. We're letting\nour agent know that\nit's done the job\nto completion. And then\nwe added full on\nbackend, frontend, end to\nend tests. Again, we're\njust stacking up. We're\nincreasing the confidence that\nour agent has done\neverything to completion without\nregression. We've then templated\nour engineering. And so\nour slash bug command\nhere, if I open\nup a new instance\nhere, right? If I\ntype slash bug, I\njust have access to\nthis template and I\ncan just quickly, you\nknow, solve any type\nof bug inside of\nmy codebase, right?\nWith just one slash\ncommand, very powerful. We're\nscaling up. We're letting\nour agents do more\nwork. And most importantly,\nwe can quickly run\nall this work in\nan off loop device\nwith our ADW tooling,\nright? With our AI\ndeveloper workflows, which we'll\nget to our big\nlast workflow here in\njust a moment. We\nthen templated our engineering.\nof Outlook systems, prompt\ninput, trigger environment and\nreview system. And you\ncan see with this\nADW, we're tapping into\nevery one of those.\nAll right. So we\nkick this off like\nthis. We're going to\nfetch the issue. We're\ngoing to run the\napplication test suite. We're\ngoing to report the\nresults. We're going to\ncommit the results and\nthen push NPR. So\nthis workflow quite literally\ntest the application and\nfixes any issues. Okay.\nAnd this is very\npowerful, right? It's completely\nseparate from the build\nand plan step. If\nyou remember the plan\nstep creates our spec\nslash tour or it'll\nrun slash feature or\nslash bug. And so\nthat's the plan step.\nThe build then takes\nthe plan and actually\nimplements. Okay, right. So\nthis runs our slash\nimplement with a path\nto our plan. And\nthen ADW test does\na few things. It\nruns a different type\nof Agentic workflow. Let's\ngo ahead and collapse\nand let's go ahead\nand walk through the\nkey elements of this.\njust at a high\nlevel. Let's walk through\nthe main of the\nAI developer workflow for\ntesting. Of course, you\ncan run this on\nyour device. It'll run\nthe entire workflow. But\nthe big idea, right,\nwe are Agentic coding\nand we want to\nget more work into\nour agents hands. The\nidea is that we'll\nrun this off device\nwith a prompt we\nkicked off at the\nbeginning of the lesson,\nright? You can see\nhere, we take a\nGitHub issue number and\nan ADW ID so\nthat we can chain\ntogether these AI developer\nworkflows. We do some\nsetup. We set up\nour new state variable.\nWe get the repo,\nright? And then it's\njust a bunch of\nsetup work here, but\nthe real work comes\nhere. here, when we\nstart running our test\nsuite, we're going to\nrun tests with resolution.\nAnd so if we\nclick into this, we\nhave this high level\nmethod that's going to\nrun tests. And so\nit's going to run\nall these tests a\nnumber of times. And\nif we go into\nrun tests, everything comes\ndown to a reusable\nprompt, right? As cloud\ncode calls these a\ncustom slash command run\ntest does this, let's\ngo ahead and open\nthis up, right slash\ntest MD purpose here.\nSo run front end\nback in tests and\nreturn a standardized JSON\nformat so that we\ncan automate agents. All\nright, so let's skip\nover some of the\ninstructions. You can of\ncourse dial into this\ncodebase, fully understand\nit, copy it, make\nit your own. But\nthe key pieces are\nhere, right? Front end\ntest, back end test,\nand then report in\nthis format. Okay, so\nwe're running our back\nend tests, just like\nyou saw before. A\ncouple more details, nothing,\nyou know, massively changed\nhere. All right, we're\ncompiling some files, running\nrough, running PyTest. All\nright, same thing for\nthe front, TypeScript compile.\nand then we're building\nthe front end, right?\nWe're adding more validation\nlayers. Great. We're then\nreporting in this JSON\nstructure. Why are we\nreporting in this JSON\nstructure? Well, so that\nwe can hand off\nany issues that arise\nto individual agents, right?\nBy getting concrete feedback,\nwe can pass these\ninto full on new\nisolated agents with their\nown context window. I\nhope you can see\nhow things are starting\nto work together. I\nhope you're seeing some\ncommon patterns. You can\ndo anything you want\nin the ADW. This\nis where you compose\nyour prompts, your Agentic\nprompts, the new world\nof engineering with the\nold world, right? Just\ndeterministic code. After we\nrun these tests, right,\nwe have our test\nresponses, we parse the\nresults, and then we\ntry to resolve any\nfailed tests. And what\ndo you think the\nfailed tests do? Of\ncourse, they do one\nthing. They run a\ntemplate, resolve failed test.\nresolve failed test, right?\nSo we're going to\nget that failed test\ninput that was created\nby our top level\ntesting agent. And then\nwe're going to fire\noff a new individual\nagent right here. Okay,\nyou know, we're looping\nthrough all the failed\ntests and kicking off\nindividual agents to resolve\nthe failed test. Okay,\nso this is super\nimportant. We can parallelize\nthis, we can do\na lot of things\nhere with the architecture\nof our ADW. The\nkey here is we\nhave a mechanism inside\nthe codebase to\nrun our tests to\nrun every test we\nhave. And then to\nresolve the issues, right?\nAnd it's isolated. It's\nnot part of the\nbuild. It's not part\nof the plan. This\nruns on its own\nwith its own set\nof agents in its\nown context windows. Okay.\nAnd so this is\nwhy it's so important\nto adopt our agent's\nperspective. We want to\nknow what's inside of\nevery agent at any\ntime. And so we\nhave one more key\npiece of this, of\nthe output formats. You\ncan see exactly what\nthis is reporting for\nevery end to end\ntest that runs. All\nright. So that's this\nand we're looping through\nhere. We're running on\nevery one of our\nend to end test\nfiles. As you can\nsee, right, I know\nI'm kind of moving\nthrough this quickly, but\nthere's a lot here.\nReally take the time,\ndial into this file,\nunderstand what's going on.\nBut the key here\nis it's actually quite\nsimple. We're taking all\nof our end to\nend tests that we're\ngoing to be building\nup as we stack\nup the work we're\ndoing, the useful work\nwe're doing for users\nin this codebase.\nAnd then we're going\nto be able to\nquickly test them end\nto end. All right.\nSo this is a\nway to scale what\nyour agents can do,\nit's a way to\nscale the work is\ntested and you'll notice\nit's gonna run every\nsingle test, it's gonna\nrun every single test\nand then try to\nresolve any issues agentically.\nSo this is our\nrun end to end\ntests. that's in run\nend to end test\nwith resolution. So we're\ngonna loop through end\ntimes and try to\ncorrect any issues if\nthere are any. It's\nall controllable with our\nretry parameters here at\nthe top. And you\ncan see several different\nagents that run in\nthis workflow. So, you\nknow, previously we looked\nat our plan and\nbuild step. We now\nhave, you know, multiple\ndifferent files that we\ncan run and compose\ntogether so that our\nloops are isolated. And\nthis is powerful because\nwe want to be\nable to tap in\nto one node of\nthe software developer lifecycle\nand improve that one\nnode. So we have\nplan, we have build,\nwe have test, and\nof course we have\nthe compositions. So there's\nADW plan and here's\nADW plan. build, right?\nSo in this workflow,\nwe just fire off\nthe scripts back to\nback. This is again,\nthe importance of having\nisolated reusable scripts. We're\nusing astral UV to\naccomplish this. You can\nuse anything you want,\nclassic shell scripts, bun\nscripts, whatever you wanna\ndo, but you just\nneed to make sure\nthat these are isolated\nin their own, you\nknow, separate Agentic layer\nin your codebase,\nright? And this is\nwhat the ADW directory\ndoes for us. Of\ncourse, you can see\na couple of different\nmodules. We've broken things\ndown a little bit\nmore. We've added ADW\ntests, right? So while\nwe're building this out,\nwe of course have\nsome tests. We're getting\nreally meta now. Ultimately,\nit all builds up\nto this, right? ADW\nplan build test. And\nthis is what ran\nin the beginning. Let's\ngo ahead. You can\nsee we have, you\nknow, the exact steps\nbuilt up there. We\nare working our way\nthrough the software development\nlifecycle with our agents.\nAnd if we open\nup the browser here,\nwe can check on\nthat feature. So inside\nthis issue, you can\nsee we have a\nPR attached and we\nhave a bunch of\nwork recorded. And you\ncan see this workflow\ntook some time and\nit ended up with\none test failure. And\nit looks like, so\nwe got a lot\nof great tests completed.\nTest complex query. We\ngot a Claude Code\nerror. This is probably\na API error. Okay,\nso this is a\nClaude Code internal error,\nwhich of course will\nbreak the tests. Everything\nneeds to run on\nClaude Code here. And\nif we want to,\nwe can always dig\ninto the logs to\nvalidate. I'll bet this\nwas a overloaded API\nerror on the Anthropic\nside. Let me just\ngo ahead and confirm\nthat. We can dig\ninto the logs here.\nThis will be a\ngood exercise as well.\nLet's go into our\nagent box. and let's\nlook up that exact\nADW ID right here.\nWe can see this\nat the top of\nthe file here, starting\nworkflow ID. This is\nour AI developer workflow\nID. We can copy\nthat and then we\ncan quickly search that.\nAnd this is the\nimportance of observability and\ntracing. You wanna know\nexactly what's going on\ninside of our code\nbase. In these code\nbases, we have this\nagent directory as well\nas our logs directory\nthat trails and traces\neverything that's happened. We're\ngonna be talking about\ndocumentation and reviewing and\nsubsequent lessons, but let\nthis environment all on\nits own. So if\nwe open up the\napplication, you can see\nwe have that new\nfeature generate random query.\nIf we click this,\nwe have the work\ncompleted, right? You see\nthe loading state has\nthe button a little\noff to some additional\ntesting steps would be\ngood here to find\nthis, but it's doing\nthe work. We can\nhit query. Let's go\nahead and see what\nthis generates for us.\nAnd nice, so it\ngenerated quite a complex\nquery there to give\nthis result. Obviously we\nhave test data here,\nso it's not gonna\nbe super sharp, but\nwe can generate random\nqueries over and over\nand over. This is\npretty cool, all right?\nAnd this all happened\noff device with our\npowerful Outloop system that\nbuilt, planned and tested.\nAnd so now you\nand I, the human\nengineer comes in here\nat the end and\nmakes sure that everything\nlooks good. It's the\ntest on top of\nthe test, right? This\nis higher confidence. This\nis improved engineering. More\nand more, you can\nsee we're shipping end\nto end. There are\na lot of key\nbig ideas here. We\nare generating plans with\nMeta prompts. We've embedded\ntesting into our daily\nwork, right, into our\nbug fixes, into our\nfeatures, into our chores.\nBut then we pushed\nit even further. into\nADWs, right? So we're\ncomposing the 12 leverage\npoints of Agentic coding\nstep by step by\nstep. Even after you\nhave ADWs, you can\nstack them together, of\ncourse, to complete the\nsoftware developer lifecycle and\nyou can run them\nin isolation to improve\nthem, right? So what's\nnext? right? We're doing\na lot here. We're\nmoving fast. We're building\nup this powerful Agentic\nlayer around our code\nbase. There's a lot\ngoing on here. In\nlesson six, we conclude\nour epic journey to\nautomate this software developer\nlifecycle. We are almost\nthere. We need two\nmore key steps. We\nneed the review step\nand the document step.\nAlong the way, we're\ngoing to clarify and\nreally walk through what\na complete Agentic layer\naround our codebase\nlooks like. We have\nall the foundations. We\nhave all the big\nheavy hitting ideas. from\nhigh level prompts, low\nlevel prompts, plans, templates,\nmeta prompts, scaling all\nthe way up to\nADWs where we compose\nworkflows and get them\noff our device. We\nknow about the Peter\nframework we can use\nto run Outloop Agentic\ncoding systems. And of\ncourse, whenever we need\nto, we can hop\ninside the loop, go\nhands on and resolve\nthings very quickly. You\ncan see here this\nbug on our main\nbranch right on this\ndevice was fixed. It\nreran all of our\ntests and it made\nsure that we don't\nhave this problematic issue\nhere. So we can\ncome in here, refresh,\nand that table is\nno longer showing up.\nEverything's looking great here.\nThat was resolved. Our\nagents are doing more\nand more for us.\nSo successful products grow,\nthey become complex. We\nneed agents to keep\ntrack of everything as\nthey begin to operate\nour codebase. In\nlesson six, we conclude\nour epic journey along\nthe software developer lifecycle\nwhere our agents will\nnearly be operating our\nentire codebase themselves.\nAnd so I know\nthis is a lot.\nWe're moving quickly along\nthis software developer lifecycle.\nWe need to pack\nthis in so we\ncan get to some\nbig key Tactical Agentic\ncoding in our advanced\nlessons along lessons six,\nseven, and eight. There's\nlots to say again,\nwe glossed over some\ndetails, but it's all\nhere in the code\nbase. Definitely check this\nout, spend some time\nhere, understand what the\nAgentic layer of your\ncodebase can look\nlike. Start building these\nvalidation loops into your\ncodebase, stand up\nconcrete AI developer workflows.\nThey don't have to\nbe complex or intricate.\nEverything starts with a\nsingle prompt. All right.\nAnd everything starts in\nthe loop and then\nslowly move it out\nthe loop. Get yourself\na dedicated environment so\nthat you can operate\nout the loop. We're\nrolling. We're making progress.\nI'll see you in\nlesson six where we\nclose the loop on\nthe software developer lifecycle.\nThere we're going to\nteach our agents how\nto not just build,\nplan and test, net\nnew features and fixes\nto your codebase,\nbut we're going to\nshow them how to\nreview and document. Code\nbases grow. They evolve.\nAs we add features,\nwe need to update\ndocumentation. And as we\nship, we need to\nreview the work completely.\nWe've already taken a\nbig step into that.\nIn the next lesson,\nwe continue it and\nwe automate the software\ndeveloper lifecycle end to\nend. We're giving our\nagents full control over\nthe codebase. Great\nwork here. Take some\ntime to digest these\nideas and I'll see\nyou in Lesson 6.",
  "summary": "This lesson introduces the fifth tactic: Always Add Feedback Loops. The most valuable contribution engineers make is the experience created for users - and testing ensures that experience works as designed. Agentic coding enables agents to test at scales humans never could. The lesson covers Closed Loop Prompts - prompts that include validation steps so agents correct their own work. It demonstrates backend testing with PyTest, frontend testing with Playwright for visual validation, and combining both in ADWs. Key insight: testing is a critical leverage point of agentic coding. By structuring test results as JSON, failed tests can be passed to isolated agents with fresh context windows to fix issues. The goal is to hand off testing responsibility to agents while maintaining confidence through multiple validation layers.",
  "key_concepts": "1. ALWAYS ADD FEEDBACK LOOPS: The fifth tactic. Your work is useless unless tested. The ultimate test is your users, but now agents can validate your entire codebase with regression and end-to-end tests.\n\n2. CLOSED LOOP PROMPTS: Prompts that include validation steps so agents correct their own work. Build prompts that ensure your agents test and verify before completing, so you dont have to.\n\n3. TESTING AS A LEVERAGE POINT: Testing is a critical leverage point of agentic coding. Hand off testing responsibility to agents - they can test at scales you never will achieve.\n\n4. BACKEND TESTING: Use PyTest or similar frameworks. Run tests as part of your ADW workflow. Structure test results as JSON so failures can be passed to isolated agents for fixing.\n\n5. FRONTEND/VISUAL TESTING: Use Playwright for visual validation. Agents can take screenshots, compare UI states, and verify frontend changes match expectations. Visual testing closes loops that unit tests cannot.\n\n6. MULTI-LAYER VALIDATION: Combine multiple validation approaches - backend tests, frontend tests, TypeScript compilation, linting. Each layer adds confidence. More compute equals more confidence.\n\n7. JSON STRUCTURED FEEDBACK: Report test results in JSON format. This enables passing concrete feedback to isolated agents with fresh context windows to fix specific issues.\n\n8. REGRESSION PREVENTION: Run full test suites to prevent regressions. Every change should pass existing tests before being considered complete.\n\n9. AGENT SELF-CORRECTION: When tests fail, agents can iterate and fix issues themselves. The closed loop means agents keep working until validation passes or report specific blockers.\n\n10. SCALING REVIEW VELOCITY: By having agents test their own work, you increase review velocity. You spend less time manually verifying and more time on high-value decisions."
}