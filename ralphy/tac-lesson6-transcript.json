{
  "video_id": "let-your-agents-focus",
  "url": "https://agenticengineer.com/tactical-agentic-coding/course/let-your-agents-focus",
  "title": "Let Your Agents Focus: Agentic Review and Documentation",
  "channel": "Agentic Engineer",
  "duration": 3400,
  "language": "en",
  "language_name": "English",
  "is_auto_generated": false,
  "extracted_at": "2026-01-14T03:35:49.419Z",
  "transcript": "Welcome to lesson six\nof Tactical Agentic Coding.\nyour first of three\nadvanced lessons. In this\nlesson, we're focused on\nthe last two steps\nof the software developer\nlifecycle, review and document.\nWith these two steps,\nwe nearly arrive at\nthe future where you\ncan run a single\nprompt that triggers a\nfleet of agents that\nship your work end\nto end. We're not\njust talking about any\nagents though. We're using\nClaude Code agents that\nrun your AI developer\nworkflows to ship with\nyour expertise in your\nproduct to solve your\ndomain specific problems. With\njust the three steps\nwe've covered, you will\noutperform any cloud-based agentic\ncoding tool, but we're\nnot done yet. After\nyou complete this lesson,\nnot only will you\nknow how to plan,\nbuild, and test autonomously,\nyour agents will be\nable to review their\nwork to make sure\nwhat they've done is\nexactly what you've asked.\nIf the work doesn't\nalign with your plan,\nthey'll agentically fix the\nissue. If it does,\nthey'll give you a\nconcise summary of the\nwork just like you\nor a team member\nwould to concretely communicate\nwhat's been done and\nhow it works. I've\ndistinguished this step in\nthe software developer lifecycle\nfor this reason. For\nAgentic Coding, this is\nmassively important for increasing\nyour review velocity. After\nreview, your agents will\nthen take a critical,\noften neglected final step.\nThey'll document their work.\nThis creates a complete\nsoftware developer lifecycle feedback\nloop that makes agents\nyou run in the\nfuture more performant because\nthey know exactly what\ncame before and when\nto include a given\npiece of documentation. So\nyou might be thinking,\nwhat's the difference between\ntesting and reviewing? Why\ndo we have this\nextra review step. Some\nengineering teams group these\ntogether. I think that's\na massive mistake, especially\nin the age of\nagents. Here's why. Every\nstep of the software\ndeveloper lifecycle can be\nrepresented as a question\nand an answer. For\ninstance, the plan step\nasks, what are we\nbuilding? The build step\nasks, did we make\nit real? Testing says,\ndoes it work? The\nreview step asks, is\nwhat we built what\nwe planned. And then\nof course, document asks,\nhow does it work?\nNotice testing answers the\nquestion, does it work?\nBut review answers the\nquestion, is what we\nbuilt what we asked\nfor? So by reviewing,\nwe're not talking about\ncode quality or implementation\ndetails. We're handing all\nthat off to our\nagents. We're asking a\nspecific set of agents\ntrained to review this\nquestion is what was\nbuilt what we asked\nfor now prove it\nthe question and the\nanswer is essential this\nis key for increasing\nyour review velocity you\nmay have already run\ninto the constraints of\nAgentic Coding planning and\nreviewing here in TAC\nsix we're going to\nimprove on the review\nconstraint you'll see exactly\nwhat that looks like\nin a moment these\ntwo final steps can\ndrastically increase your agent\nto coding KPIs. We\nwant attempts down, streak\nup, size up, presence\ndown. So after we\nknow our application works,\nthanks to our testing\nAI developer workflow, we\ncovered in our previous\nlesson, lesson five, and\nwe know the application\ncontains work we asked\nfor, thanks to the\nreview AI developer workflow\nwe'll explore in this\nlesson, we can then\nDocument the work that\nwas done. Documentation is\nsimple, but how you\ndocument and when to\ninclude your documentation is\nthe tricky part we'll\nsolve in this lesson.\nWhy do we document?\nBecause inside the new\nagentic layer of your\ncodebase, documentation provides feedback\non work done for\nfuture agents to reference\nin their work. They\ncan operate and then\nupdate the documentation when\nthe time is right.\nGreat. So we know\nwhat we're doing, but\nwhat's the tactical advantage\nwe're learning in this\nlesson. This lessons tactic,\nsolves many issues engineers\nface when working with\nagents today this tactic\ndrives great agentic development\nat scale because it\nforces us to make\na controversial decision and\ncommit to it in\nreturn we completely sidestep\na massive list of\nAgentic Coding potholes and\nproblems the tactic is\none agent one prompt\none purpose This unlocks\nmassive Agentic Coding capabilities\nthroughout the new agentic\nlayer of your code\nbase. Why is that?\nIsn't more specific context\nbetter for your agent,\nthe more it knows\nthe better it performs?\nThis is not true.\nMassive context windows often\nleads to a distracted,\nconfused, agent. This is\ncontext pollution. This is\ncontext overloading. This is\ntoxic context, wherever you\nwant to call it.\nWhen you overload the\ncontext window, your agent\nhas a harder time\nfocusing on what matters\nthe most, the original\ntask you asked it\nto complete. And yes,\ngenerative AI companies, big\ntech, they're chasing the\nall-in-one, god model the\nsuper agent that can\ndo it all but\nthat's not what we're\ndoing here we're solving\nreal engineering problems with\nour boots on the\nground with real work\nahead of us there's\na reason why they're\ncalled ai developer workflows\nbecause they add compute\nto your developer workflows\nthe workflows you take\nevery day as an\nengineer you and i\nas engineers we operate\none step at a\ntime and every step\nof engineering requires a\ndifferent set of information\na different approach a\ndifferent perspective it requires\na different set of\ntools and context okay\nthink of a large\nfeature you recently shipped\neven with in-loop agentic\ncoding it doesn't happen\nin one swing there\nare steps there's process\nthere's planning building testing\nreviewing and documenting all\nright so we're breaking\nthese down step by\nstep. This is the\npower of the AI\ndeveloper workflow. You get\nto now encode this,\nright? You get to\ntemplate your engineering and\nhand off the work\nto your agents. You\ncan teach your agents\nhow to operate your\ncodebase to solve\nyour problems. Real engineering\nrequires real developer workflows,\nnot some pie in\nthe sky God model\nthat claims to do\nit all. And, you\nknow, let me just\nspoil it for you.\nThey won't, it's not\nhere. It's not coming\nsoon. The real engineering\nstill must happen. One\nagent, one prompt, one\npurpose. You want to\nuse specialized agents with\nfocus prompts to achieve\na single purpose. You've\nalready seen this in\nthe TAC codebases.\nNo agent runs more\nthan a single prompt.\nNow this doesn't mean\nthe prompt isn't large\nor the prompt doesn't\ndo a lot of\nwork, right? You've seen\nthis in lesson three,\nfour, and five. We\nhave meta prompts. We\nhave templates. We have\nhigher order prompts. These\nare not simple ideas.\nWe're not doing simple\nsets of work. We're\nnot sparing our agent.\nor the language model\nfrom complexity. What we're\ndoing has many advantages.\nLet's run through the\nadvantages of specialized one\nprompt agents. Why is\nthis so powerful? You\nfree up the context\nwindow. You give your\nagent the full 200K,\n500K, 1 million tokens,\nwhatever you're working with,\nwhatever Agentic Coding tool\nyou have, and whatever\ntime you're in, you\ngive your agent all\nof the space and\nthe opportunity to solve\nthe one problem well.\nThis is important as\nyour codebase grows.\nTo execute work, your\nagent will likely need\nto pull in more\nand more context. As\nagentic engineers, we have\nthree constraints, the context\nwindow, the complexity of\nour codebase, the\nproblem we're solving and\nlastly our abilities specialized\nagents bypass two out\nof three of these\nall right model intelligence\nis not a constraint\ndon't use this as\nan excuse it will\nset you back this\nis a losing mindset\nokay when you free\nup the context window\nIt means you can\nalso solve bigger problems\nin your workflows. And\nit also simplifies your\nworkflows. We work one\nstep at a time,\none agent at a\ntime, one prompt, one\npurpose at a time.\nAll right. The compact\ncommand inside of Claude\nCode, maybe this will\nbe resolved in the\nfuture, but this is\na bandaid fix. If\nyour agent is running\ncompact, it is losing\ninformation. Okay. So what's\nanother key advantage of\nthis tactic? One agent,\none prompt, one purpose.\nIt lets your agent,\nagent focus. This is\na very similar advantage,\nright? This should be\nrelatable for you as\nan engineer. A focused\nengineer working on a\nsingle task is a\nproductive engineer. Agents are\nthe same. Big context\nwindows cause context confusion.\nThis drops your agent's\nperformance. And when your\nagent's performance drops, your\nperformance drops. So let\nyour agent's Focus. Every\npiece of context you\nadd increases the number\nof variables your agent\nhas to reason about.\nYou want to context\nengineer as little as\npossible. You want the\nminimum context in your\nprompt required to solve\nthe problem. When you\nadopt your agent's perspective,\nit should be clear\nwhat context it has,\nthe available tools, and\nhow the problem can\nbe solved with the\nfocused leverage points it\nhas available. Last but\nnot least, one agent,\none prompt, one purpose\nhas a very special\nside effect. You likely\nalready know what this\nis. You're seeing it\nin this codebase.\nWe get to commit\nevery one of our\nprompts, all of our\nworkflows, which means we\ncan easily improve them,\nokay? Since you're not\nstacking up many prompt\ncalls from who knows\nwhere, we can easily\nreproduce and more importantly,\nimprove every single step\ndown to the prompt\nlevel. This is big.\nWhile other engineers and\ncompanies are running big\ncontext windows that'll be\nblown away at the\nend. You'll be running\nsingle focus prompts that\nyou can pinpoint directly\nand improve. You've seen\nthis in the code\nbase. We just search\nfor the slash command,\nopen up the file,\nand we know what\nour agent can see,\nall right? When you're\nrunning in loop, right,\nhands on the keyboard,\nyou can quickly iterate\nand run your prompts\nso that they're battle\ntested to run out\nthe loop with your\nAFK agent system. By\ndoing all this, you\neffectively create evals for\nthe agentic layer of\nyour codebase. Whoa,\nwhoa, whoa. Evals for\nyour agents in your\ncodebase. Yes, this\nis a big idea.\nWe're not gonna explore\nit much more here\nin TAC. We have\nother priorities that are\nmore important, but individualized\nagents, specific prompts, one\npurpose, you can evaluate\nthat. Okay, you can\nchange the model. You\ncan add thinking mode.\nYou can change your\nAgentic Coding tool. You\ncan rerun these over\nand over. You can\nget checkout before you\nran work and run\nit. right, you can\nliterally create evals for\nyour codebase, which\nmeans you accelerate on\nthe path to improving\nyour agentic KPIs that\nmuch further. And all\nthat aside, you will\npassively, you will start\nforming evals in your\nhead just passively as\nyou build up the\nagentic layer of your\ncodebase. You'll understand\nwhat works well and\nwhat doesn't work. So\nas we work through\nthe review and document\nsteps in this code\nbase and the tactics\ncodebase, notice how\nwe're using this tactic\nthroughout the codebase.\nOne agent, one prompt,\none purpose, one, one,\none. Let's dive into\nthe codebase to\nsee how we can\nbuild dedicated agents to\nreview and document agentically.\nAll right, so let's\nopen up the terminal.\nBusiness as usual here.\nIf we run LS,\nwe can see the\nprevious five codebases.\nLet's clone in the\nLesson 6 repository, link\navailable in your loot\nbox below. I'm gonna\nCDN. and we're gonna\nboot up your favorite\neditor. I'm gonna use\ncode here. So right\naway, we're going to\nboot up Claude. I'm\ngoing to once again\nuse Yolo mode and\nI'm gonna fire up\nthe Opus model here.\nJust as before, we\ncan run our all-in-one\nset of command slash\ninstall. This is a\ngreat way to encode\nyour engineering workflows for\nyou and your team.\nIf we open up\nslash install, we have\none addition here at\nthe bottom. we are\ngoing to set up\nsome Cloudflare environment variables\nso that we can\nupload some image assets.\nThis is gonna be\nreally important for our\nreview workflow. This is\na little teaser of\nwhat's coming next. Aside\nfrom that, it's the\nexact same workflow. We're\ngonna copy in some\nvariables, reset the database\nand boot up our\nserver in the background.\nOur agent is setting\neverything up for us.\nThere is no reason\nto do this work\nby hand. You have\nagents use them. So\na couple things right\naway while this is\nrunning in the background,\nlet's go ahead and\ntake a look at\nour new AI developer\nworkflows. This is the\nnew agentic layer around\nyour codebase. And\nin order for this\nto work, it composes\nyour prompts, okay? But\nyour workflows detail exactly\nwhat the capabilities are\nfor your agents. Let's\nlook at our new\nlow-level ADWs that can\nbe chained together. We\nhave a brand new\nreview. We have a\nbrand new patch. and\nwe have a document\nand you can see\nwe have some composed\nmethods that put them\ntogether. So we're going\nto be running plan,\nbuild, review and plan,\nbuild, document. We also\nmight just run the\ndocument step on its\nown after we fire\noff plan, build, review.\nWhat does this do?\nWhat are we gonna\nbe looking at here,\nright? So all of\nthese composed workflows, they\ndo exactly what you\nthink they do. They\nrun the lower level\nADW, plan, build, review.\nAnd so if we\njust look through these,\nright, we look at\nADW, we can see\nexactly what's happening here,\nright? We have plan\nrunning in a UV\nsingle file script build\nand review. It's really\nimportant that these are\nstandalone scripts. We can\nthen compose them and\nmake them run side\nby side together. And\nthe ADW ID is\nthe thing that changed\ntogether the state of\nall of their work.\nSo this is plan,\nbuild, review. All the\nother composite workflows, the\ncomposed workflows look the\nexact same. In plan,\nbuild, document, we have\nthat exact same thing,\nADW underscore, and then\nwe have plan, build,\ndocument. Okay? It's important\nto note that you\nneed something to test\ndocument and review against.\nSo plan and build\nare our essential methods.\nIt's the work that\nallows the other three\nsteps to even exist.\nChecker agent looks like\nit's setting up our\ndatabase. Great. And there's\ngoing to start the\nserver in the background.\nThat looks great. We've\ncovered plan and build\nin previous lessons. Let's\nwhat was built is\nnot what we asked\nfor, resolve the issues\nand make up the\ndifference, post the results\nand push and update\nthe PR. OK, so\nthat's what this review\ndoes. So without even\nlooking at the code,\nright, without even looking\nat the steps, we\nknow exactly what this\ndoes. Let's see if\nour codebase is\nset up. Fantastic. It\nis. You can always\nreview the required steps.\nSo you can see\nwe have some manual\nsteps here. I'm going\nto work through these\nquickly, get this all\nfilled out, set the\norigin back to the\nTAC six codebase.\nYou're going to want\nto, of course, fork,\nset it to a\nnew codebase. So\nyou have access to\nGitHub issues and get\nup PRS set remote\npaste, skip push, filled\nout some of these\nmissing variables. So I'm\ngoing to go ahead\nand say start application\nagain, already just handing\noff work. to our\nagent so it can\ndo it. There's no\nreason for us to\ndo this anymore. And\nso if we refresh,\nyou can see our\napplication is in a\ngreat state. Select five\nusers, fire it off.\nAnd we'll of course\nget five users here.\nFantastic. So our application\nhere is back up\nand running. Let's go\nahead and create a\nnew GitHub issue. This\nis our prompt source.\nSo let's go ahead\nand create an issue\nhere and fire off\na new workflow. Create\nissue. We want a\nnew feature for our\nnatural language SQL interface.\nLet's use something a\nlittle bit more interesting\nhere. So one click,\ntable exports. I'm just\ngoing to paste this\nprompt in here using\nthe ADW workflow. Add\none click table exports\nand one click result\nexport feature to get\nresults as CSV files.\nWe want two new\nendpoints. I'm specifying where\nwe want this to\nbe triggered. Then I'm\nsaying, you know, use\nthe right download icon.\nSo this is a\nhigh level prompt that's\nmoving into mid-level territory.\nWe do have some\nengineering details here. Feel\nfree to add as\nmuch detail to communicate\nexactly what you want\ndone. You want these\nto mostly be high\nlevel prompts because we've\ntemplated our engineering already\ninside of our planning\nprompts, right? Our slash\ntour slash bug slash\nfeature slash whatever class\nof problem you want\nto solve. So I'm\ngoing to go ahead\nand create this. And\nnow we have an\nissue ID that we\ncan use to kick\noff our workflow. As\nyou can see here,\nI don't have my\ndedicated agent environment. We're\ngoing to use my\ndevice. We're going to\nlet our agents run\non this device to\nget work done so\nthat we can analyze\nand understand what's going\non under the hood\nin real time. So\nin a new terminal\nhere, we can run\nUV run plan, build,\nreview. I'm gonna copy\nthe relative path to\nthis paste and let's\nget our issue number.\nWe have issue 14\nhere and we don't\nneed an ADW ID\nsince this is a\nnew workflow. One will\nbe created for us.\nThat should be good.\nSo we're gonna fire\nthis off. Now our\nworkflow is running. So\nour agents have now\ntaken over this repository.\nWe shouldn't be doing\nanything alongside them. They\nare running their own\nworkflows designed to ship\nend to end. You\nknow, we're running plan,\nbuild, review while this\nis running. Let's understand\nthe review workflow at\na deeper level so\nthat we know exactly\nwhat's going on here.\nWhenever you want to\nunderstand one of these\nworkflows, go right to\nthe main method. Let's\nrun this top to\nbottom and just focus\non the key pieces\nof information. All right.\nSo we're passing in\narguments. We're setting up\nour state. We have\nto make sure we\nhave the right state\nto run this workflow.\nAnd if we open\nup the file explorer\nhere, collapse everything and\nopen up agents, you\ncan see we have\nthis ADW ID with\nevery dedicated agent, right?\nAll their logs is\ngetting placed here. And\nwe have something really\nimportant. We have state\nmeta information that travels\nalong every one of\nour AI developer workflows.\nIt's really simple. As\nyou can see here,\nADW ID key fields,\nbranch name, plan file,\nissue class. Okay, our\nworkflows validate that the\nstate that they need\nexists from the state\nfile, So, you know,\nwhenever you see state\nthat is exactly what's\nhappening here. We're checking\nour environment variables, getting\nthe GitHub repo from\nthe state object and\nthen we're validating, right?\nYou can't review if\nyou don't have work\nto review and you\ndo work on new\nbranches. So you can\nsee here, we're just\nvalidating the branch, making\nsure we have that\nchecked out. And then\na lot of all\nthe code here is\njust logging and communicating\nto our Peter review\nsystem. Remember the Peter\nframework, we have prompt\ninput, trigger environment and\nour review system. Our\nreview system is of\ncourse GitHub issues here.\nIf we scroll down,\nyou can see we're\nwriting updates. Right now\nour agents are working\non the implementation plan.\nWe find the spec\nfile again, coming out\nof the state. We\nlog and then here\nwe have something really,\nreally cool. We are\ninitializing our R2 uploader.\nSo we're going to\nupload images based on\nour review to a\nCloudflare R2 bucket. This\nof course can be\nany bucket you can\nimagine. We just need\nthis for publicly hosted\nimages so that we\ncan attach them on\nour GitHub issues or\non the PR, wherever\nyou'd like to place\ntools to come into\nthis workflow and to\nunderstand what went wrong\nif we need to\nand to apply patch\nfixes, which is what\nthe ADW patch is\nfor. Okay, but always\nlean toward the future.\nWe're not here to\nstay in the loop\nforever, right? They're called\nOutloop systems for a\nreason. You want your\nagent shipping end to\nend. And if something\ngoes wrong, you fix\nthe end to end\nportion, not the thing\nthat went wrong, right?\nThis is the tricky\npart. And this is\nthe important transition we\nand I have to\nmake as agentic engineers.\nOkay, but let's look\ninto the run review\nand guess what it's\ndoing. It's really simple.\nIt is running the\nreview prompt. You know,\nyou can always just\nsearch and understand where\nall the prompts are\ngetting fired off. You\nknow, search all slash\ncommand equals template request,\nexecute template, right? It's\nall there in the\ncodebase. Very easy\nto find. Let's look\nat the review prompt.\nEverything comes down to\nthe prompt level. All\nthe ADW surrounding deterministic\ncode is just support\nfor the agents doing\nthe real work. So\nwhat does review do?\nOf course, you know,\nwe have a, you\nknow, growing library of\nprompts that drive our\nagents behavior. And in\nthe review prompt, we're\ndoing a couple essential\nthings. If we just\ncollapse everything, we can\nsee a simple view\nof this prompt and\nunderstand it at a\nhigh level. There's always\na great place to\nstart when you're reviewing\nprompts, collapse, keep it\nsimple. Don't get overwhelmed.\nFollow the instructions, review\nthe work done against\na specification file, right?\nUse git diff, understand\nthe changes that were\nmade, capture screenshots and\nreport any issues. Okay.\nIf not report success.\nGreat. What are we\nreporting exactly? You can\nsee our variables here,\nADW spec file, agent\nname, review image directory.\nSo we're being really\nclear, right? You can\njust create variables at\nthe top of your\nprompts that'll then get\nused as long as\nyou're referencing them. What\nare we doing here?\ninformation dense keywords on\nthe fly. These are\nvariables that can be\nused and are used\nthroughout the prompt, right?\nIf we search for\nADWID, you can see\nit getting used there.\nIf we search for\nagent name, of course,\nyou can see that\nthere. And of course,\nour review image directory,\nthis is getting referenced\ndirectly in our instructions.\nWhat does the output\nformat look like? Of\ncourse, you can look\nat the instructions there.\nHere's the report, right?\nThis is the interesting\npart. We are outputting\nJSON from our review\nprompt. Of course, you\nwant to leverage types.\nThis is a key\nleverage point of Agentic\ncoding. So everything we're\ndoing here has a\ncorresponding type. If we\njust search for review\nresult, you can see\nwe have this type\nexactly, right? Success, review\nsummary issues, screenshots, screenshot\nURL, yada, yada, yada,\nright? And it contains,\nof course, a nested\ntype review issue. The\nimportant thing to call\nout here is that\nin a review issue,\nwe're looking for three\nclasses of issue. Okay.\nWe have skippable tech\ndebt and blocker. You\nwant to make sure\nyou're accounting for things\nthat your agents are\ngoing to find in\nyour work when they're\nreviewing your code and\nyour application. Okay. So\nwe don't want to\nblock on skippable or\ntech debt. We want\nto block on blockers.\nOkay. So we're communicating\nthis to our agents.\nThey will find something\nif you ask them\nto. All right, these\nagents are very agreeable,\nbut they're also just\ngreat at engineering. They're\nplan looks like this.\nIf we open up\npatch, you can see,\nthis right here, right?\nAlways collapse and just\nlook at a high\nlevel, create a focus\nto patch plan, resolve\na specific issue. So\nwe're making a surgical\nchange here. Follow the\ninstructions to create a\nconcise plan to address\nthe issue with minimal\ntargeted changes. And so\nthis is a really\npowerful prompt. Of course,\nwe can't just do\neverything with our ADW\nfirst shot. We're going\nto need to come\nin and improve things\nto patch issues. So\nwe have a concise\npatch plan. This is\nyou know, to be\nsuper clear, very similar\nto our chore prompt\nand our bug and\nour feature prompt as\nwell, right? So these\nare a template meta\nprompts. They're going to\noutput a plan that\nwe can then execute\nafter doing research. So\nthese are all dedicated\nprompts that run single\nagents that do one\nthing well, right? They\nhave a single purpose.\nOkay. And so you\ncould very easily see\nwhen you're writing these\nsmall patches, that's very\ndifferent than a chore,\na bug, or a\nfeature, right? We're just\nchanging one thing. We\nwant a simple quick\nfix. This is how\nwe do it. Just\nto mention it here,\nADW patch does exactly\nthis, okay? It runs\nthis workflow, right? So\ncreate and implement patch.\nAnd so the first\nthing we do is\ncreate a patch plan,\nand then we run\nour implement plan. And\nwe all know what\nimplement does. It runs\nthe slash implement agentic\nprompt. We can open\nthat up, of course,\nas well. And you'll\nsee we are starting\nto reuse our prompts.\nThis is a higher\norder prompt. It takes\nin a plan, right?\nA path to a\nplan. We're going to\npass it in. And\njust like our plan\nfrom chore bug or\nfeature is going to\ndo that exact same\nworkflow. It's going to\nread the plan, think\nhard and implement. Okay,\nso that's it. So\nyou can see we're\nstarting to get some\nreuse of the compositional\npieces of our code\nbase. This is really\nimportant. And again, we're\nhitting on that theme\nof reusability and single\nagents with one prompt\nwith a single purpose.\nOkay, so this is\npowerful after the implement\nplan runs through. Let's\nclose all these. We\ngo back up and\nwe have the patch\nfile and the implementation\nresponse. And of course,\nwe're just doing additional\nlogging, looking for success,\nlooking for failures. And\nif there's a failure,\nwe try again. Okay,\nso that's what ADW\nreview does. Let's check\nin with our workflow.\nIt looks like we\nare still building. That\nlooks great. And whenever\nyou need to, you\nknow, you can hop\nup to the issue\nor whatever your prompt\nreview system is, and\nyou can just see\nwhat's going on, right?\nSo you can see\nwe are still implementing\nthe solution. Having a\nreview system is really\nimportant. As we discussed\nin lesson five, you\nwant to be operating\nout the loop over\nthe long term. As\nyou improve here as\nan agentic engineer, it\nwill become better and\nbetter and better at\noperating the codebase\nwithout you because you've\nput in the work\ninto the agentic layer\ncovering the software developer\nlifecycle. So you can\nsee here, it took\nabout eight minutes for\nour agents to implement\nthe solution. We should\nhave a commit and\na PR following here\npretty soon. This will\nupdate the existing PR,\nwhich right now will\njust contain the plan\nfor this, right? Thanks\nto the plan step\nin our software developer\nlifecycle, AI developer workflow.\nReview issue, come back\nup to the top.\nAgain, do a bunch\nof reporting a lot\nof this code just\nto make it super\nclear. It's just reporting,\nright? Look at all\nthese hits on the\nsidebar here. This is\njust reporting. Most of\nthis code is just\ncommunicating what was done.\nThis is super important.\nYou always want to\nbe communicating the work\nthat you've done to\nyour review system so\nyou can easily review.\nAnd just to mention\nit again, you know,\nIt doesn't matter what\nyou use for your\nreview system, right? Maybe\nyou like GitHub issues.\nMaybe you want to\nput it on the\npull requests itself. Maybe\nyou want to use\na entirely different resource,\nright? Jira. I can't\nimagine someone wants to\nuse Jira, but you\nknow, you might want\nto use something like\nsome Jira, some type\nof task management board,\nwhatever you want to\ndo. do it your\nway. All we're doing\nhere, I'm not showing\nyou how to do\nit exactly. I'm showing\nyou what you can\ndo. Keep the Outloop\nsystem in mind and\nalways know, of course,\nthere are gonna be\ntimes when you need\nto come into the\nloop, work in the\nterminal, fire up instances\nthat you're prompting back\nand forth. And it's\ngreat for proving your\nworkflows and making sure\nthat they work. So\nlike for instance here,\nyou know, in my\ninstall script, it looks\nlike my agent kicked\noff start on its\nown, which locked up\nits loop. So this\nis something that we\ncan improve in the\ninstall command, right? Make\nsure to be running\non the background process.\nAnd to be fair,\nI probably should have\njust communicated that in\nthe inline prompt. Anyway,\nso right now our\nagents are creating the\nimplementation commit and you\ncan always dial in\nbecause it has full\ncontrol over this repository,\nright? If we want\nto, we can see\nits work, git diff\ndash dash stat. and\nwe wanna run this\nagainst probably origin main.\nAnd so you can\nsee all the changes\nit's made so far\nagainst the origin main\nbranch. So you can\nsee there's our plan.\nIt created a nice\ntest for us there,\nso on and so\nforth. It has our\nend-to-end tests, it's creating\nthis. This is part\nof our engineering. We've\nencoded it into our\ntemplate. So this stuff\njust happens all the\ntime now. right? We've\ntemplated our engineering and\nour agents know what\nto do with this\nstuff, right? We're preparing\nfor, you know, that\nworld where we are\njust staying out the\nloop. These are all\ntactics that you should\nbe thinking as you're\noperating with your agents,\nright? You should be\nthinking these things, right?\nTemplating my engineering so\nthat I can stay\nout the loop, building\nsystems that have one\nagent, one prompt with\na single purpose. Okay.\nIf we look at\nthe agents, AWID directory,\nwe can see that\nwe are reviewing the\ncode now. I'm really,\nreally excited to show\nyou what's coming next.\nYou can see here\nR2 upload enabled and\nwe have this public\nbucket that we're going\nto have access to.\nAnd you can kind\nof imagine what's going\nto happen next, assuming\nthat this workflow works\nas intended, assuming our\nagent ships the work\nthat was designed to\nship. You never know.\nThings can always go\nwrong. These are non-deterministic\nsystems. But the goal\nis to make it\nso that we have\nan entire pipeline of\nagents that are checking\neach other's work, making\nsure things look good\nand communicating to us\nif that's not the\ncase, and then we\ncan improve our composable\nunits. And so this\nis the review step.\nWe have, you know,\nkind of three distinct\nagents that are running\nhere. It's a good\npractice to encode the\nagent names as constants\nright at the top\nof the file. So\nit's super clear what\nagents are running, what\nagents are controlling that\ngiven workflow. We can\nalways just search one\nof these, you know,\noutput logs, and we\ncan see exactly where\nour agent is at\nin this workflow. So,\nyou know, we are\nreviewing against the specification.\nRight. So super important\npoint running the review,\nuploading images next. And\nthen we're going to\nresolve any blocking issues\nif any existed. Okay.\nIt looks like our\nworkflow has been completed.\nPlan build review. So\nup here we can\nsee something incredible. Okay.\nWe have images uploaded.\nAnd so we can\nclick into this and\nwe can see right\non our public URL\nhere, we have that\ndown arrow. Okay. And\nso there it is\nthere. It looks like\nit is off here.\nSo our agent did\nnot obey this properly,\nright? The export should\nnot be in the\ncenter there. And we\ncan run a patch\nto clean this up.\nThis is a perfect\nexample of needing to\nhave a concrete review\nstep and improve the\nreview step. So, you\nknow, we can see\nthat and our agent\nis reporting one click\nfeature export has been\nimplemented implementation, blah, blah,\nblah, blah, blah, error\nhandling all tests passed.\nAs you can see,\nhere, there is work\nto be done on\nimproving the prompt and\nthe workflow for this\nreview process, right? We\nhave our export button\nright here and we\nasked for it to\nbe right here. Okay.\nSo let's go ahead,\nput in a patch\nfor this. And, you\nknow, of course we\ncan, let's go ahead\nand open up the\ncode as well. So\nI'm going to Huffactor\nagent here, I'm gonna\ntype slash start and\nit's going to run\na start command to\nagentically kick off the\nclient servers. It's gonna\nkick it off again,\nthere we go. And\nso you can see\nwe have our download\nhere. So if we\nclick this, very cool.\nSo if we click\nthis and if we\nclick download product, we\nshould see the same\nthing, great. We have\na download there, select\nfive users, just running\na random query here.\nAnd this is where\nour issue is, okay?\nSo yeah, so this\nis right, the wrong\nformatting. This should be\nright next to the\nhide button. If we\nclick this, you can\nsee we do get\nthe download which is\ngreat, but of course\nour UI is a\nlittle off. So this\nis a great example\nto showcase the patch\nprompt. We will run\ninto issues. We need\na mechanism inside of\nour workflows to quickly\nknock them out, ideally\nout the loop. Just\nreal quick, I'm going\nto type ADW patch.\nI'm going to operate\nthis like we're working\noutside the loop just\nfor good practice. I'm\ngoing to copy the\nADW ID, paste that\nthere, and then we're\ngoing to say, what\nare we patching? make\nsure download button for\nquery results. And that\nshould be it, right?\nSo a simple patch,\nI'm gonna write a\ncomment here and now\nI'm gonna run our\npatch workflow. Remember everything\nruns from the prompt\nsource. So that's there,\nI'm gonna open up\nhere and then let's\ngo ahead and kick\noff this new workflow.\nThis is gonna be\nUV run and now\nwe're gonna run a\nW patch, copy, place\nthat there. We need\nthe issue ID, it's\ngonna be 15. And\nwe also want the\nADW ID, right? We're\nnot starting new work,\nwe're building off existing\nwork. So I'm gonna\npaste this in. That's\ncaught this. Our review\nis just looking to\nsee if the feature\nis there. We always\njust have to be\nready for a model\njust to blatantly make\na mistake like this.\nAnd we look at\nthis result, right? Like\nplacing this here is\ndefinitely something our agent\nshould have caught. How\ncould we have improved\nthis? There are a\ncouple of places to\ntackle, right? we're running\nthe plan, build and\nreview. So we can\nreally attack any one\nof these surface areas.\nThe review step really\nshould have caught that.\nOkay, so what's something\nwe can do here\nin the review step?\nYou know, something that\nis just completely missing\nhere. Our agent is\ncapturing these images and\nsomething that we can\nadd here that could\nbe really, really important.\nThe actual screenshots never\nmake it into the\nagent. To be super\nclear here, our agent\nis operating Playwright. If\nyou look at the\nroot directory here, we\nhave this .mcp.json and\nwe have a simple\nPlaywright configuration that allows\nour agents to operate\nPlaywright. And so something\nthat we missed here\nand something that isn't\nactivated is we don't\nhave this dash dash\nvision. We're not operating\nin vision mode. The\ntokens aren't flowing through\nthe vision capabilities of\nour language model. So\nthis is something we\ncould add to tweak\nthe workflow. I don't\nlike this because this\nis outside of the\nADW. So I want\nto kind of stay\naway from that and\njust keep general configuration\nhere. Something we could\nadd here is inside\nof the instructions for\nevery image you take,\nbe sure to read\nit in during the\nreview process. Okay. And\nso This is something\nthat we could very\nwell add. And then\nour agent would likely\nread the image, have\nit in its context\nwindow, and then be\nable to improve on\nthis small issue here,\nright? Because it was\nreally just that one\nUI tweak that caused\nthe issue, right? Let's\nsee how our prompt\npatch is going, right?\nThis is the whole\npoint of the patch\nworkflow. We have a\npatch plan created. This\nis getting created inside\nof our specs directory.\nAnd it looks like\nour fix was just\nfinished. So our patch\njust completed. But you\ncan see here inside\nof specs where we\nnormally just have plans,\nwe now have this\npatch directory. Okay. And\nso again, we are\nrecording every large piece\nof work that occurs.\nThe biggest piece of\nwork is the actual\nimplementation work. So we\nalways want to have\na plan that details\nexactly what gets done.\nSo this is the\noutput of our patch,\nright? So just to\nbe super clear, this\npatch prompt, right? This\npatch agentic prompt creates\nthis patch plan, super\nsimple, super concise. And\nyou know, you can\nsee it has detailed\nthe issue summary and\nthe solution. Okay. Not\ncorrectly positioned, create a\nbutton container to group\nexport and hide right\nside of the results\nheader. Okay. And so\nwe should be able\nto just, you know,\nrefresh, select random product.\nAnd now we should\nsee this. There we\ngo. Export is now\nright next to hide.\nAnd you know, I\nknow I absolutely can\njust hear a couple\nof engineers watching this\nnow. Why didn't you\njust do that by\nhand? You're missing the\npoint. Okay. The whole\npoint here is that\nwe are putting in\nthe effort to build\nthe system that builds\nthe system. Yes. Like\nI know that you\nand I can use\nthese gigantic tools. We\ncan even do it\nby hand if we're\nbeing really dumb. We\nboth know that we\ncan fix this instantly\nwith a prompt. Okay.\nThat's not the point.\nThis isn't about you\nanymore. Okay, this is\nabout your agents. It's\nabout teaching your agents\nhow to build on\nyour behalf, right? So\nthat it can be\nabout you again, okay?\nOur hands, our minds,\nremember tactic one, stop\ncoding. That is going\nto start pushing into\nsome of these hands-on\npieces of engineering, like\nstop these one-off low-level\nprompts, okay? You really\nwanna extend that as\nmuch as you can,\nright? Teach your agent\nhow to do it.\nWe can, of course,\ndownload this one row.\nNot a big deal\nthere, right? Download, download.\nThis all works great.\nWe can of course\nopen up. There is\nour products in a\nnice CSV file. The\nfeature obviously worked. The\nimportant piece was implemented\nfor us end to\nend. So this is\ngreat, right? This is\nthe importance of having\nreview. Now, you know,\nsomething really important that\nI wanna call out\nabout this review process.\nThanks for our Outloop\nsystem, we were able\nconcrete improvement. I can\nmake to the review\nprocess, right? And this\nis something that you\nwould want to do,\nright? For every image\nyou take, be sure\nto read in, read\nit in during the\nreview process. And then,\nyou know, something like\nthis, right? To make\nsure it matches the\nspec. and the original\nrequest. Question I can\nask myself as I'm\nbuilding the system that\nbuilds the system, do\nI have the original\nrequest? If you remember\nright at the top\nhere, I have these\ninstructions, right? Place a\nblank, blank, blank directly\nto the left of\ntheir respective thing, right?\nDirect to the left\nof the hide button.\nSo this means I\ncan ask myself, did\nI have the original\nrequest inside the plan\nwhen it got implemented?\nAnd so, you know,\nwe can just look,\nright? We have all\nthe artifacts we need,\njust let's just look,\nright? So where is\nthat feature here? This\nis 14 and it\nlooks like I did.\nSo this was here\nand it looks like\nit is up to\nour review process to\njust be clear about\nthis. Okay. And if\nwe just search left\nhere, this was there,\nright? So our agent\njust missed this, right?\nIt just missed this\ndetail. And so this\nis something that is\njust going to happen,\nright? As agents improve,\nas tools improve, this\nwill happen less, but\nthis happened here. It's\nalso really important to\nnote. I don't wanna\nkeep harping on this\ntoo long, but it's\nimportant to note that\nwe did completely skip\nthe test step, right?\nWhich operates and validates\nend to end, right?\nSo we skipped the\ntest step. Our export\nfunctionality was not run,\nokay? And so you\ncan see here, we\ndo have that instruction.\nOur agent was very\nclear here. Make sure\ndownload appears to the\nleft of the X\nicon, to the left\nof the hide button.\nSo again, you wanna\nbe really careful with\nthe steps you have\nin your software developer\nLifecycle, testing is there\nfor a reason, right?\nThis is a great\nshowcase of, you know,\ntesting very likely would\nhave caught this issue,\nright? If we look\nat our ADWs here,\nremember we ran plan\nbuild review. We completely\nskipped plan build test\nreview, right? We could\nhave run this workflow\nhere, right? This is\nprobably the winning workflow.\nLet's move on to\ndocumentation. So these are\nall composable. So what\nwe're gonna do here\nis just append document.\nWe're gonna document this\nfeature right at the\nend here, okay? So\nI'm just gonna open\nthis up, UV run,\npaste the file path,\nof course, run against\nthe issue ID, and\nI am going to\nrun on the same\nworkflow, right? The document\nstep needs existing work.\nI'm gonna copy the\nADW ID, paste that,\nand let it kick\noff, okay? So this\nis our documentation step.\nYou can see there's\nover the place. We\nadded export. If we\ngo into markdown preview\nmode here, we're also\ngoing to get our\nscreenshots. Okay. So there's\na screenshot there and\nwe didn't rerun the\nreview step. So we\nhave our previous export\nthere. This is an\nimprovement we want to\nmake to our workflow.\nThat's great. If we\nlook through this, you\ncan see exactly what\nwas built, exactly what\nwas changed. All the\nfiles here to support\nthis changes, modified, clear,\nconcise documentation, how to\nuse it, so on\nand so forth. Right.\nand allows us to\nautomate some of the\nwork that engineers just\ndon't do or work\nthat chews up a\nlot of engineering time,\nright? Agentic documentation, agentic\nreview is ultra, ultra\npowerful. And now we\nhave this piece of\ndocumentation, okay? So this\nis powerful, but this\nis not all that\nwas done. Remember in\nthe beginning, I mentioned\nthat there are two\nsteps to documentation, creating\ndocumentation for the work\nyou've done and then\nthere's knowing when and\nhow to pull in\nthe documentation. So if\nwe open up the\ndocumentation prompt here, not\nonly is it a\ntemplate meta prompt, but\nit is also going\nto do something special.\nIt's going to update\na prompt, okay? So\nwe're gonna update our\nconditional documentation. What is\nthis exactly? What is\nconditional documentation? Let's go\nahead and open this\nup. Let's take a\nlook at this. So\nwe have a prompt\nhere, right? A lower\nlevel prompt that is\nused in high level\nprompts like the document\nprompt. And this is\ngoing to conditionally add\ndocumentation to the high\nlevel prompt, right? To\nthe agent calling the\nhigh level prompt. So\nthis prompt helps you\ndetermine what documentation you\nshould read based on\nspecific changes you need\nto make in the\ncodebase. Review the\nconditions below and read\nthe relevant documentation before\nproceeding with your tasks.\nSo it's quite simple,\nright? We're telling our\nagents, read this documentation\ngiven a condition. And\nso you can see\nhere, our agent has\nagentically updated this file\nfor all future agents\nreferencing this file, right?\nAnd here comes the\nmagical part with agentic\ndocumentation. If we search\nall for conditional docs,\nguess where this is\ngonna be? Guess where\nyou're gonna find this?\nInside bug, inside chore,\ninside feature, inside patch,\nand even inside our\nprime command, okay? This\nis very, very powerful.\nAnd if we go\ninto, let's say we're\nfixing a bug, right?\nIt's inside the relevant\nfile section, right? Let's\ncollapse everything else. We\ncan get a high\nlevel overview, right? Our\nbug is our template\nmeta prompt that is\ncreating a plan to\nfix a bug. But\nif we look at\nthe relevant files here,\nwe're saying, read this\nAnd then we documented\nall the work that\nwas done and we\nupdated our conditional documentation,\nwhich tells all future\nagents inside of our\nbug, inside of our\nfeature, right, patch tells\nall future agents running\nplans for new work,\nnet new engineering work.\nIt tells them what\ndocumentation they should read\nin. If we open\nup conditional docs here,\nit tells them when\nto pull in pieces\nof documentation. And so\nhere we are, we\nhave completed the software\ndeveloper lifecycle. Back at\nthe issue, we have\nthis really, really powerful\nfeature where our agents\nare presenting the work\nto us. Okay, this\nis super, super critical\nfor agenting engineering work.\nYou don't want to\nbe in the loop.\nYou don't want to\nbe on the ground\nand opening up the\napplication. If you need\nto, of course you\ncan, right? Go lower\nlevel when you need\nto, go into the\napplication when you need\nto. But here we\nhad our agents just\npresent us with the\nproof that they have\nreviewed the system. Now,\nthe review was wrong.\nThey should have caught\nthis. We did say\nto the left of\nthe hide button. So\nthere are some improvements\nto make on our\nside, okay? I'm not\nblaming the model for\nthis. I'm blaming my\nskills. I'm blaming the\nADWs we have set\nup right now, okay?\nThat's the problem here.\nBut notice how quickly\nwe were able to\ncatch this with proof\nof value, right? Agentic\nproof of value. Review\nanswers the critical question.\nIs what we built\nwhat we asked for?\nEven when your agent\nis wrong, when it\npresents you with proof\nthat it thinks it's\nright, you can quickly\nsay Yes or no.\nNot all engineers are\nworking on front-end systems.\nIf you're building language\nmodels, data analytics work,\nbuilding scikit models, old\nschool scikit models, right?\nClassification models. Maybe you're\ndoing some rapid prototyping,\nright? Whatever you would\ndo to review that\nthe thing is what\nyou asked for, whatever\nsteps you would take,\nthis is what you're\ngoing to embed into\nyour review step. This\nis what the review\nstep does for you.\nOkay. This is the\nreview step. This is\nADW review. You want\nto encode for this\nspecific codebase for\nthe problem that you're\nsolving. You want to\nencode how you would\nreview to answer the\nquestion is what was\nbuilt, what we asked\nfor. Okay. And so\nyou want some type\nof hard asset that\nproves that what was\nbuilt is what you\nasked for. So it\ncould be some type\nof report. It could\nbe, you know, for\nUI, like we have\nhere, it is a\nimage. It also can\nbe a video. You\nwant to take this\nto the next step.\nWhat you want is\na live asset that\nyou can review that\nproves that what was\nbuilt is what was\nasked for. And this\nis super, super critical\nbecause it directly increases\nyour review velocity. And\nthat means your agentic\ncoding KPIs. And so\nI hope you can\nsee that you want\nto put all these\nsteps together. They're more\npowerful together. When we\nlook ahead to what's\nnext, really important to\nmention that the value\nin your ADWs isn't\nin a single workflow,\nright? It's the power\nof combining workflows, having\nthem separate yet working\ntogether. You want them\nisolated. You want them\nreviewable. You want them\nimprovable, right? And this\nis why we have\none agent working on\na single problem with\na single purpose, right?\nOne agent, one prompt,\none purpose. What are\nthe benefits of this?\nYou free up the\ncontext window. You can\nlet your agent focus\non a single problem.\nYour prompts become improvable\nas well as reusable,\ninvisible, of course. And\nlastly, you create a\nsystem where your code\nbase can be easily\nevaluated, right, in a\ndeep technical sense and\njust passively as you\nwork and build the\nagentic layer of your\ncodebase. So now\nwe can agentically review\nand document. This completes\nthe software developer lifecycle.\nSo in our next\nlesson, we're going to\nput this stuff to\nwork, right? I fully\nrealized that we have\nbeen operating on this\nkind of simple toy\nexample. That's fine, right?\nObviously we're not focused\non what is getting\nbuilt. There isn't, enough\ntime for me to\ncover all the different\ncodebase types and\ncodebase applications that\nevery engineer watching this\nis operating in. There\njust isn't enough time.\nSo this exactly doesn't\nmatter. But what does\nmatter here, and in\nour next lessons, we're\ngonna showcase this. What\nmatters is the velocity\nyou can achieve with\nan agentic layer of\nyour codebase like\nthis. I'm gonna show\nyou how quickly you\ncan really ship in\nour next lesson, okay?\nIt's going to be\nabsolutely mind blowing. I\nwanna really nail home\nthe point that you\ncan't compete with this,\nokay? So that's what\nwe're gonna do in\nour next lesson. Every\nlesson, every tactic, every\nprompt we've run, in\nloop, out loop, it's\nall leading up to\nthis big reveal of\nTactical Agentic Coding, right?\nI'm gonna reveal the\nsecret of Tactical Agentic\ncoding. Understanding the secret\nis mission critical to\nagentic layer, right? We\ncan fix the small\nissue that we had\nwith the positioning, right?\nWe can encode that,\nwe can template that\ninto our engineering. All\nthe work we're doing\nhere is going to\nimprove your Agentic Coding\nKPIs. This is how\nwe know we're improving\nour Agentic Coding capabilities.\nThis is what all\nthese steps in the\nsoftware developer lifecycle helps\nus achieve, right? That's\nwhy we built the\nagentic layer. We want\nattempts down, size up,\nstreak up, presence down.\nAs a concrete example\nof this, we ran\none prompt at the\nbeginning to add export\nfunctionality to our tables\nand our response queries.\nAnd we had an\nissue at the beginning\nof this execution, right?\nWe just ran that\none prompt. We had\none attempt, right? This\nis the optimal state\nfor your attempt KPI.\nWe then ran the\npatch that moved our\nattempts up to two.\nThis is not good.\nBut on the bright\nside, the work that\nwe did ship with\nthat one prompt was\nsolid, right? We were\nable to export CSV\nfrom our tables and\nfrom the query result.\nSo size was good\nthere. We did everything\nwe wanted to in\nthe right flow, right?\nWe did break our\nstreak because we needed\nto call two AI\ndeveloper workflows to ship\nthe feature. Okay. So\nour streak was broken,\nright? Reset back to\none. Okay. And how\nis our presence? Obviously\nI'm operating here in\nthe loop to showcase\ntactics of Agentic Coding,\nbut how was our\npresence, right? Very quickly,\nwe were able to\nidentify an issue thanks\nto our review images\nand then quickly patch\nit. So our presence\nwas something like three.\nWe were there in\nthe beginning. We saw\nthis issue. We ran\nthe patch and then\nwe reviewed it again.\nIdeally, you want two\nmoments of presence, one\nat the beginning for\nyour prompt input, one\nat the end for\nyour review and your\nPR merge. That's the\noptimal case. As you'll\nsee in our next\nlesson, you can actually\npush that further. This\nis where things are\ngoing to start to\nfeel and get a\nlittle wacky as engineers.\nIn addition to the\nsecret of Agentic Coding,\nwe're going to move\neven faster, just like\nyou can with individual\nagent instances. You can\nrun entire AI developer\nworkflows in dangerous mode,\nwhere we ship all\nthe way to production.\nWe're gonna showcase exactly\nwhat that looks like\nand how and when\nwe would use this,\nbecause if you're on\nthe edge of this\nstuff, I think you\ncan start to see\nhow and when we\ncould use this, not\nfor big features, right?\nNot for serious work\nlike that, but for\nmaybe some bugs, maybe\nsome chores, we can\nstart to run entire\nworkflows in yellow mode\nbut i'll see you\nin lesson seven where\nwe dive in to\nthe secret of Tactical\nAgentic Coding the secret\nof Agentic Coding great\nwork here do not\nwait to start putting\nthis stuff in your\ncodebase set up\nbasic adws set up\nbasic agentic prompts get\nyour advantage start rolling\nthis into your code\nbases the value here\nis immense i'll see\nyou in lesson seven\nwe're going to reveal\nthe secret of agentic\nengineering",
  "summary": "This advanced lesson covers the final two steps of the software developer lifecycle: Review and Document. The key tactic is One Agent, One Prompt, One Purpose - keeping agents focused on single tasks rather than overloading context windows. The lesson explains context pollution (too much context leads to confused, distracted agents) and why minimum viable context is optimal. It introduces Review Agents that verify work matches the plan and either fix issues or provide concise summaries. Documentation Agents create artifacts for future agents to reference. The lesson also covers Conditional Documentation - prompts that dynamically pull in relevant docs based on the changes being made. By the end, agents can plan, build, test, review, and document autonomously.",
  "key_concepts": "1. ONE AGENT, ONE PROMPT, ONE PURPOSE: The sixth tactic. Keep agents focused on single tasks. A focused agent is a productive agent. Every piece of context increases variables the agent must reason about.\n\n2. CONTEXT POLLUTION: Too much context leads to confused, distracted agents. Big context windows cause context confusion which drops agent performance. Use minimum context required to solve the problem.\n\n3. REVIEW AND DOCUMENT: The final two steps of the software developer lifecycle. With all five steps (Plan, Build, Test, Review, Document), agents can ship work end-to-end autonomously.\n\n4. REVIEW AGENTS: Specialized agents that verify work matches the original plan. If work doesnt align, they agentically fix the issue. If it does, they provide a concise summary of what was done.\n\n5. DOCUMENTATION AGENTS: Create artifacts and documentation for future agents to reference. Documentation is feedback on work done that helps future agents understand context and make better decisions.\n\n6. CONDITIONAL DOCUMENTATION: Prompts that dynamically determine what documentation to pull in based on specific changes being made. Keeps context minimal and relevant.\n\n7. ADOPT YOUR AGENTS PERSPECTIVE: It should be clear what context your agent has, the available tools, and how the problem can be solved with focused leverage points. Context engineer as little as possible.\n\n8. COMMIT YOUR PROMPTS: One agent, one prompt, one purpose means every prompt can be committed to the codebase. This makes prompts easy to improve, version, and share across the team.\n\n9. OUTPERFORM CLOUD TOOLS: With Plan, Build, Test, Review, and Document steps automated, you outperform any cloud-based agentic coding tool because your agents understand YOUR codebase and domain.\n\n10. FLEET OF AGENTS: The goal is running a single prompt that triggers a fleet of specialized agents that ship work end-to-end using your expertise, your product knowledge, and your domain-specific solutions."
}