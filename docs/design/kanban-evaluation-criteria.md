# Kanban Solution Evaluation Criteria Design

> **PRD Phase 6 Task**: Design evaluation criteria for comparing solutions from parallel iterations

## Overview

This document defines the evaluation framework for comparing solutions generated by parallel agent iterations in the Autonomous Kanban Agent. When multiple agents work on the same task simultaneously, the system needs objective and subjective criteria to select the best solution or synthesize multiple approaches.

## Design Principles

1. **Objectivity First**: Prioritize automated, measurable criteria over subjective assessment
2. **Multi-Dimensional**: Evaluate across multiple orthogonal dimensions
3. **Confidence-Aware**: Track confidence in evaluations, not just scores
4. **Learnable**: Enable the system to improve evaluation over time from user feedback
5. **Transparent**: Provide clear explanations for rankings and selections

## Evaluation Categories

### 1. Correctness (Weight: 40%)

Measures whether the solution actually solves the stated problem.

**Automated Checks:**

| Criterion | Method | Score Range |
|-----------|--------|-------------|
| Tests Pass | Run existing test suite | 0.0 - 1.0 |
| Type Check | `tsc --noEmit` exit code | 0.0 or 1.0 |
| Lint Clean | `pnpm lint` exit code | 0.0 or 1.0 |
| Build Success | `pnpm build` exit code | 0.0 or 1.0 |
| No Regressions | Compare test counts before/after | 0.0 - 1.0 |

**Semantic Checks:**

| Criterion | Method | Score Range |
|-----------|--------|-------------|
| Requirement Coverage | LLM analysis of task vs solution | 0.0 - 1.0 |
| Edge Cases | LLM check for common edge cases | 0.0 - 1.0 |
| API Compatibility | Check for breaking changes | 0.0 or 1.0 |

```typescript
interface CorrectnessScore {
  testsPass: number;          // Ratio of passing tests
  typeCheck: boolean;         // tsc passes
  lintClean: boolean;         // No lint errors
  buildSuccess: boolean;      // Build completes
  noRegressions: boolean;     // No test count decrease
  requirementCoverage: number; // 0-1, LLM assessed
  edgeCaseHandling: number;   // 0-1, LLM assessed
  apiCompatible: boolean;     // No breaking changes

  // Aggregated score (0-1)
  overall: number;
}
```

### 2. Code Quality (Weight: 25%)

Measures the maintainability and clarity of the solution.

**Automated Metrics:**

| Criterion | Method | Ideal Range |
|-----------|--------|-------------|
| Cyclomatic Complexity | Static analysis | < 10 per function |
| Lines of Code | Delta from baseline | Minimal increase |
| Duplication | Clone detection | 0% duplicated |
| File Size | LOC per file | < 500 LOC |
| Function Length | Lines per function | < 50 lines |

**Heuristic Assessments:**

| Criterion | Method | Score Range |
|-----------|--------|-------------|
| Naming Quality | LLM review | 0.0 - 1.0 |
| Comment Coverage | Ratio of comments to code | 0.0 - 1.0 |
| Pattern Adherence | Match existing codebase patterns | 0.0 - 1.0 |
| Error Handling | LLM check for proper error handling | 0.0 - 1.0 |

```typescript
interface QualityScore {
  complexity: {
    average: number;         // Avg cyclomatic complexity
    max: number;             // Max in any function
    score: number;           // Normalized 0-1 (lower is better)
  };
  size: {
    linesAdded: number;
    linesRemoved: number;
    netChange: number;
    score: number;           // Normalized 0-1
  };
  duplication: {
    percentage: number;
    score: number;           // 1 - duplication%
  };
  naming: number;            // LLM assessed 0-1
  comments: number;          // Ratio-based 0-1
  patternAdherence: number;  // LLM assessed 0-1
  errorHandling: number;     // LLM assessed 0-1

  overall: number;
}
```

### 3. Efficiency (Weight: 15%)

Measures performance characteristics of the solution.

**Runtime Metrics (when applicable):**

| Criterion | Method | Score Range |
|-----------|--------|-------------|
| Execution Time | Benchmark comparison | Relative to baseline |
| Memory Usage | Peak heap measurement | Relative to baseline |
| I/O Operations | Count file/network calls | Minimize |

**Static Analysis:**

| Criterion | Method | Score Range |
|-----------|--------|-------------|
| Algorithm Complexity | LLM assessment (Big-O) | 0.0 - 1.0 |
| Resource Cleanup | Check for leaks/cleanup | 0.0 or 1.0 |
| Async Efficiency | Check for unnecessary awaits | 0.0 - 1.0 |

```typescript
interface EfficiencyScore {
  runtime?: {
    baseline: number;        // ms
    solution: number;        // ms
    ratio: number;           // solution / baseline
    score: number;           // Normalized 0-1
  };
  memory?: {
    baseline: number;        // bytes
    solution: number;        // bytes
    ratio: number;
    score: number;
  };
  algorithmic: number;       // LLM assessed complexity
  resourceCleanup: boolean;
  asyncEfficiency: number;

  overall: number;
}
```

### 4. Completeness (Weight: 10%)

Measures how thoroughly the solution addresses the task.

| Criterion | Method | Score Range |
|-----------|--------|-------------|
| All Requirements Met | Checklist from task | 0.0 - 1.0 |
| Documentation Added | Doc updates if needed | 0.0 or 1.0 |
| Tests Added | New tests for new code | 0.0 - 1.0 |
| Changelog Updated | Entry added if user-facing | 0.0 or 1.0 |

```typescript
interface CompletenessScore {
  requirementsMet: number;   // Ratio of requirements addressed
  documentationAdded: boolean;
  testsAdded: number;        // Ratio of new code covered
  changelogUpdated: boolean;

  overall: number;
}
```

### 5. Safety (Weight: 10%)

Measures the risk profile of the solution.

| Criterion | Method | Score Range |
|-----------|--------|-------------|
| No Dangerous Operations | Check for risky patterns | 0.0 or 1.0 |
| Security Review | LLM security scan | 0.0 - 1.0 |
| No Secrets Exposed | Scan for credentials | 0.0 or 1.0 |
| Rollback Safe | Can be easily reverted | 0.0 - 1.0 |

```typescript
interface SafetyScore {
  noDangerousOps: boolean;   // No rm -rf, DROP TABLE, etc.
  securityReview: number;    // LLM assessed 0-1
  noSecretsExposed: boolean; // No hardcoded credentials
  rollbackSafe: number;      // How easily reversible

  overall: number;
}
```

## Scoring Rubrics

### Automated Scoring

For boolean criteria:
```typescript
function booleanToScore(value: boolean): number {
  return value ? 1.0 : 0.0;
}
```

For ratio-based criteria:
```typescript
function ratioToScore(actual: number, target: number): number {
  return Math.min(1.0, actual / target);
}
```

For inverse metrics (lower is better):
```typescript
function inverseScore(value: number, baseline: number, worst: number): number {
  if (value <= baseline) return 1.0;
  if (value >= worst) return 0.0;
  return 1.0 - (value - baseline) / (worst - baseline);
}
```

### LLM-Based Scoring

For subjective criteria assessed by LLM:

```typescript
interface LLMAssessment {
  score: number;           // 0.0 - 1.0
  confidence: number;      // 0.0 - 1.0
  reasoning: string;       // Explanation for the score
  suggestions?: string[];  // Potential improvements
}

const LLM_SCORING_PROMPT = `
Evaluate the following code change on a scale of 0.0 to 1.0 for {criterion}.

Context:
- Task description: {taskDescription}
- Original code: {originalCode}
- Solution code: {solutionCode}

Scoring guide:
- 0.0-0.2: Poor - Major issues present
- 0.2-0.4: Below average - Several issues
- 0.4-0.6: Average - Acceptable with some issues
- 0.6-0.8: Good - Minor issues only
- 0.8-1.0: Excellent - No significant issues

Respond with JSON:
{
  "score": <number>,
  "confidence": <number>,
  "reasoning": "<explanation>",
  "suggestions": ["<improvement1>", "<improvement2>"]
}
`;
```

## Comparison and Ranking Algorithm

### Individual Solution Score

```typescript
interface SolutionEvaluation {
  solutionId: string;
  iterationId: string;

  correctness: CorrectnessScore;
  quality: QualityScore;
  efficiency: EfficiencyScore;
  completeness: CompletenessScore;
  safety: SafetyScore;

  // Weighted aggregate
  overallScore: number;

  // Confidence in the evaluation
  confidence: number;

  // Evaluation timestamp
  evaluatedAt: Date;
}

function calculateOverallScore(eval: SolutionEvaluation): number {
  const weights = {
    correctness: 0.40,
    quality: 0.25,
    efficiency: 0.15,
    completeness: 0.10,
    safety: 0.10,
  };

  return (
    eval.correctness.overall * weights.correctness +
    eval.quality.overall * weights.quality +
    eval.efficiency.overall * weights.efficiency +
    eval.completeness.overall * weights.completeness +
    eval.safety.overall * weights.safety
  );
}
```

### Multi-Solution Ranking

```typescript
interface SolutionRanking {
  solutions: RankedSolution[];
  winner: RankedSolution | null;
  confidence: number;
  comparisonDetails: ComparisonDetail[];
}

interface RankedSolution {
  solutionId: string;
  rank: number;              // 1 = best
  score: number;
  evaluation: SolutionEvaluation;
  strengths: string[];       // Key advantages
  weaknesses: string[];      // Key disadvantages
}

interface ComparisonDetail {
  solutionA: string;
  solutionB: string;
  winner: string | "tie";
  category: string;
  scoreDiff: number;
  reasoning: string;
}

function rankSolutions(evaluations: SolutionEvaluation[]): SolutionRanking {
  // Sort by overall score descending
  const sorted = [...evaluations].sort(
    (a, b) => b.overallScore - a.overallScore
  );

  // Build rankings with analysis
  const solutions: RankedSolution[] = sorted.map((eval, index) => ({
    solutionId: eval.solutionId,
    rank: index + 1,
    score: eval.overallScore,
    evaluation: eval,
    strengths: identifyStrengths(eval),
    weaknesses: identifyWeaknesses(eval),
  }));

  // Generate pairwise comparisons
  const comparisons = generatePairwiseComparisons(evaluations);

  // Calculate confidence in the ranking
  const confidence = calculateRankingConfidence(solutions);

  return {
    solutions,
    winner: confidence >= 0.6 ? solutions[0] : null,
    confidence,
    comparisonDetails: comparisons,
  };
}
```

### Confidence Calculation

```typescript
function calculateRankingConfidence(solutions: RankedSolution[]): number {
  if (solutions.length === 0) return 0;
  if (solutions.length === 1) return 1;

  // Factor 1: Score gap between #1 and #2
  const scoreGap = solutions[0].score - solutions[1].score;
  const gapConfidence = Math.min(1, scoreGap / 0.1); // 0.1 gap = 100% confidence

  // Factor 2: Average evaluation confidence
  const avgEvalConfidence = solutions.reduce(
    (sum, s) => sum + s.evaluation.confidence, 0
  ) / solutions.length;

  // Factor 3: Consistency across categories
  const consistencyScore = calculateCategoryConsistency(solutions[0], solutions[1]);

  // Weighted combination
  return (
    gapConfidence * 0.4 +
    avgEvalConfidence * 0.3 +
    consistencyScore * 0.3
  );
}

function calculateCategoryConsistency(
  first: RankedSolution,
  second: RankedSolution
): number {
  // Check if first beats second in most categories
  const categories = ['correctness', 'quality', 'efficiency', 'completeness', 'safety'];
  let firstWins = 0;

  for (const cat of categories) {
    if (first.evaluation[cat].overall > second.evaluation[cat].overall) {
      firstWins++;
    }
  }

  return firstWins / categories.length;
}
```

## Auto-Acceptance Thresholds

Solutions can be automatically accepted without user review when confidence is high enough.

```typescript
interface AutoAcceptanceConfig {
  // Minimum overall score for auto-acceptance
  minScore: number;           // default: 0.85

  // Minimum confidence in evaluation
  minConfidence: number;      // default: 0.80

  // Minimum score in each category (hard floors)
  categoryMinimums: {
    correctness: number;      // default: 0.90 (must pass tests)
    quality: number;          // default: 0.70
    efficiency: number;       // default: 0.60
    completeness: number;     // default: 0.80
    safety: number;           // default: 0.95 (must be safe)
  };

  // Minimum gap from second-place solution
  minScoreGap: number;        // default: 0.10

  // Whether auto-acceptance is enabled at all
  enabled: boolean;           // default: false (require explicit opt-in)
}

const defaultAutoAcceptanceConfig: AutoAcceptanceConfig = {
  minScore: 0.85,
  minConfidence: 0.80,
  categoryMinimums: {
    correctness: 0.90,
    quality: 0.70,
    efficiency: 0.60,
    completeness: 0.80,
    safety: 0.95,
  },
  minScoreGap: 0.10,
  enabled: false,
};

function shouldAutoAccept(
  ranking: SolutionRanking,
  config: AutoAcceptanceConfig = defaultAutoAcceptanceConfig
): { accept: boolean; reason: string } {
  if (!config.enabled) {
    return { accept: false, reason: "Auto-acceptance disabled" };
  }

  if (!ranking.winner) {
    return { accept: false, reason: "No clear winner" };
  }

  const winner = ranking.winner;
  const eval = winner.evaluation;

  // Check overall score
  if (winner.score < config.minScore) {
    return {
      accept: false,
      reason: `Score ${winner.score.toFixed(2)} below threshold ${config.minScore}`
    };
  }

  // Check evaluation confidence
  if (eval.confidence < config.minConfidence) {
    return {
      accept: false,
      reason: `Confidence ${eval.confidence.toFixed(2)} below threshold ${config.minConfidence}`
    };
  }

  // Check category minimums
  for (const [category, minimum] of Object.entries(config.categoryMinimums)) {
    const score = eval[category as keyof typeof config.categoryMinimums].overall;
    if (score < minimum) {
      return {
        accept: false,
        reason: `${category} score ${score.toFixed(2)} below minimum ${minimum}`
      };
    }
  }

  // Check score gap (if multiple solutions)
  if (ranking.solutions.length > 1) {
    const gap = winner.score - ranking.solutions[1].score;
    if (gap < config.minScoreGap) {
      return {
        accept: false,
        reason: `Score gap ${gap.toFixed(2)} below minimum ${config.minScoreGap}`
      };
    }
  }

  return { accept: true, reason: "All criteria met" };
}
```

## Automated vs Manual Evaluation

### Fully Automated

| Criterion | Tool/Method |
|-----------|-------------|
| Tests Pass | `vitest run` |
| Type Check | `tsc --noEmit` |
| Lint Clean | `pnpm lint` |
| Build Success | `pnpm build` |
| Lines of Code | `git diff --stat` |
| Cyclomatic Complexity | Custom AST analysis |
| Duplication | Clone detection tool |
| Secrets Scan | Pattern matching |

### LLM-Assisted (Semi-Automated)

| Criterion | Prompt Template |
|-----------|-----------------|
| Requirement Coverage | Compare task description to implementation |
| Edge Case Handling | Identify potential edge cases and check handling |
| Naming Quality | Review identifiers for clarity |
| Pattern Adherence | Compare to existing code patterns |
| Error Handling | Check for proper error propagation |
| Security Review | Scan for common vulnerabilities |
| Algorithm Complexity | Assess Big-O notation |

### Manual Review Required

| Criterion | When Required |
|-----------|---------------|
| Breaking Changes | When API surface is modified |
| User Experience | When UI/UX is affected |
| Architecture Decisions | When new patterns are introduced |
| Performance Critical | When in hot path |
| Security Sensitive | When handling credentials/auth |

## Evaluation Pipeline

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        Evaluation Pipeline                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Solution Received                                                      │
│         │                                                               │
│         ▼                                                               │
│  ┌──────────────────────────────────────────────────────────┐           │
│  │  Stage 1: Automated Checks (blocking)                    │           │
│  │  - Type check                                            │           │
│  │  - Lint                                                  │           │
│  │  - Build                                                 │           │
│  │  - Tests                                                 │           │
│  │  If any fail: Score = 0, skip remaining stages           │           │
│  └──────────────────────────────────────────────────────────┘           │
│         │                                                               │
│         ▼                                                               │
│  ┌──────────────────────────────────────────────────────────┐           │
│  │  Stage 2: Metric Collection (parallel)                   │           │
│  │  - Code complexity                                       │           │
│  │  - LOC delta                                             │           │
│  │  - Duplication                                           │           │
│  │  - Test coverage delta                                   │           │
│  └──────────────────────────────────────────────────────────┘           │
│         │                                                               │
│         ▼                                                               │
│  ┌──────────────────────────────────────────────────────────┐           │
│  │  Stage 3: LLM Assessment (parallel)                      │           │
│  │  - Requirement coverage                                  │           │
│  │  - Code quality review                                   │           │
│  │  - Security scan                                         │           │
│  │  - Pattern adherence                                     │           │
│  └──────────────────────────────────────────────────────────┘           │
│         │                                                               │
│         ▼                                                               │
│  ┌──────────────────────────────────────────────────────────┐           │
│  │  Stage 4: Score Aggregation                              │           │
│  │  - Calculate category scores                             │           │
│  │  - Apply weights                                         │           │
│  │  - Calculate overall score                               │           │
│  │  - Determine confidence                                  │           │
│  └──────────────────────────────────────────────────────────┘           │
│         │                                                               │
│         ▼                                                               │
│  ┌──────────────────────────────────────────────────────────┐           │
│  │  Stage 5: Ranking (after all solutions evaluated)        │           │
│  │  - Sort by overall score                                 │           │
│  │  - Generate pairwise comparisons                         │           │
│  │  - Identify strengths/weaknesses                         │           │
│  │  - Check auto-acceptance criteria                        │           │
│  └──────────────────────────────────────────────────────────┘           │
│         │                                                               │
│         ▼                                                               │
│  Decision: Auto-accept OR Present to User                               │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

## User Presentation Format

When presenting ranked solutions to the user:

```markdown
## Solution Comparison Results

### Winner: Iteration #2 (Score: 0.87, Confidence: 92%)

| Category | #1 | #2 (Winner) | #3 |
|----------|-----|-------------|-----|
| Correctness | 0.85 | **0.95** | 0.80 |
| Quality | 0.75 | **0.82** | 0.78 |
| Efficiency | 0.70 | 0.75 | **0.80** |
| Completeness | 0.90 | **0.95** | 0.85 |
| Safety | 1.00 | 1.00 | 1.00 |
| **Overall** | 0.82 | **0.87** | 0.81 |

### Why Iteration #2?

**Strengths:**
- All tests pass with 3 new tests added
- Clean implementation following existing patterns
- Complete documentation coverage

**Trade-offs:**
- Slightly higher complexity than #1
- #3 has better runtime efficiency but incomplete tests

### Actions

- [Accept Winner] Apply iteration #2's solution
- [View Diff] See detailed changes
- [Compare Pair] Side-by-side comparison
- [Manual Review] Request human review
- [Reject All] Discard all iterations
```

## Learning from Feedback

Track user decisions to improve future evaluations:

```typescript
interface EvaluationFeedback {
  rankingId: string;
  userChoice: "accepted_winner" | "chose_other" | "rejected_all" | "manual_edit";
  chosenSolutionId?: string;
  feedbackNotes?: string;
  timestamp: Date;
}

interface EvaluationLearning {
  // Weight adjustments based on user preferences
  categoryWeightAdjustments: Record<string, number>;

  // Common patterns where evaluation was wrong
  knownWeaknesses: {
    pattern: string;
    correction: string;
  }[];

  // User-specific preferences
  userPreferences: {
    prefersReadability: number;      // -1 to 1
    prefersPerformance: number;      // -1 to 1
    prefersMinimalChanges: number;   // -1 to 1
  };
}
```

## Configuration Schema

```typescript
interface EvaluationConfig {
  // Category weights (must sum to 1.0)
  weights: {
    correctness: number;
    quality: number;
    efficiency: number;
    completeness: number;
    safety: number;
  };

  // Auto-acceptance settings
  autoAcceptance: AutoAcceptanceConfig;

  // LLM settings for assessments
  llmAssessment: {
    enabled: boolean;
    model: string;              // e.g., "claude-3-5-sonnet"
    maxTokens: number;
    temperature: number;        // Low for consistency
  };

  // Automated check timeouts
  timeouts: {
    typeCheck: number;          // ms
    lint: number;
    build: number;
    tests: number;
  };

  // Caching
  cache: {
    enabled: boolean;
    ttlSeconds: number;
  };
}

const defaultEvaluationConfig: EvaluationConfig = {
  weights: {
    correctness: 0.40,
    quality: 0.25,
    efficiency: 0.15,
    completeness: 0.10,
    safety: 0.10,
  },
  autoAcceptance: defaultAutoAcceptanceConfig,
  llmAssessment: {
    enabled: true,
    model: "claude-3-5-sonnet",
    maxTokens: 1024,
    temperature: 0.1,
  },
  timeouts: {
    typeCheck: 60000,
    lint: 30000,
    build: 120000,
    tests: 300000,
  },
  cache: {
    enabled: true,
    ttlSeconds: 3600,
  },
};
```

## Implementation Files

```
src/kanban/
├── evaluation/
│   ├── types.ts              # Score and evaluation types
│   ├── config.ts             # Configuration and defaults
│   ├── automated-checks.ts   # Build, lint, test runners
│   ├── metrics.ts            # Code metrics collection
│   ├── llm-assessment.ts     # LLM-based evaluations
│   ├── scorer.ts             # Score calculation
│   ├── ranker.ts             # Multi-solution ranking
│   ├── auto-accept.ts        # Auto-acceptance logic
│   ├── presenter.ts          # User presentation formatting
│   ├── feedback.ts           # Learning from user choices
│   └── index.ts              # Public exports
```

## Security Considerations

1. **Sandboxed Execution**: All automated checks run in isolated environments
2. **Resource Limits**: Timeouts and memory limits on all evaluations
3. **No Code Execution**: LLM assessments analyze code without executing it
4. **Audit Trail**: All evaluations and decisions are logged
5. **User Override**: User can always override automated decisions

## Testing Strategy

1. **Unit Tests**: Each scoring function tested in isolation
2. **Integration Tests**: Full evaluation pipeline with mock solutions
3. **Snapshot Tests**: Verify consistent output for same input
4. **Confidence Tests**: Verify confidence calculations are accurate
5. **Regression Tests**: Ensure ranking stability over time

## References

- Pattern confidence: `src/patterns/types.ts`
- Feedback effectiveness: `src/reminders/feedback-types.ts`
- Hybrid search ranking: `src/memory/hybrid.ts`
- AGI research: `docs/AGI_RESEARCH.md`
- Kanban store: `src/dashboard/kanban-store.ts`
